{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import clip\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(source_dir):\n",
    "    \"\"\"\n",
    "    Renames all jpg files in the source directory with their Design Labels.\n",
    "\n",
    "    Parameters:\n",
    "    source_dir: str, the path to the directory containing the jpg files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    \n",
    "    for file in files:\n",
    "\n",
    "        # Check if the file is a jpg\n",
    "        if file.endswith('.jpg') or file.endswith('.jp2'):\n",
    "\n",
    "            # Get the file extension\n",
    "            _, ext = os.path.splitext(file)\n",
    "\n",
    "            # Skip the first VL in the file name\n",
    "            first_vl_index = file.find('VL')\n",
    "\n",
    "            # Find the next VL in the file name\n",
    "            if first_vl_index != -1:\n",
    "                start_index = file.find('VL', first_vl_index + 2)\n",
    "                if start_index != -1:\n",
    "                    end_index = file.find('.', start_index)\n",
    "                    new_name = file[start_index:end_index] if end_index != -1 else file[start_index:]\n",
    "\n",
    "                    # Rename the file\n",
    "                    original_file_path = os.path.join(source_dir, file)\n",
    "                    new_file_path = os.path.join(source_dir, new_name + ext)\n",
    "                    os.rename(original_file_path, new_file_path)\n",
    "\n",
    "\n",
    "def process_filenames(folder_path):\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Split the filename into parts based on the underscore\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        # Ensure the filename has the correct number of parts\n",
    "        if len(parts) >= 4:\n",
    "            # Extract the part between the second and third underscore\n",
    "            target_part = parts[2]\n",
    "            \n",
    "            # Replace the dot with an underscore in the extracted part\n",
    "            new_part = target_part.replace('.', '_')\n",
    "            \n",
    "            # Get the file extension\n",
    "            file_extension = os.path.splitext(filename)[1]\n",
    "            \n",
    "            # Construct the new filename\n",
    "            new_filename = f\"{new_part}{file_extension}\"\n",
    "            \n",
    "            # Print or rename the file as needed\n",
    "            print(f\"Original filename: {filename}\")\n",
    "            print(f\"Processed filename: {new_filename}\\n\")\n",
    "            \n",
    "            # To actually rename the file, uncomment the next line\n",
    "            os.rename(os.path.join(folder_path, filename), os.path.join(folder_path, new_filename))\n",
    "\n",
    "# Example usage\n",
    "folder_path = '/Users/ilerisoy/Downloads/Classics/models'\n",
    "process_filenames(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted VL_FI_VL00792.206_R_00.jp2 to VL_FI_VL00792.206_R_00.jpg\n",
      "Converted VL_FI_VL03636.136_R_00.jp2 to VL_FI_VL03636.136_R_00.jpg\n",
      "Converted VL_FI_VL00022.311_R_00.jp2 to VL_FI_VL00022.311_R_00.jpg\n",
      "Converted VL_FI_VL00578.112_R_00.jp2 to VL_FI_VL00578.112_R_00.jpg\n",
      "Converted VL_FI_VL00633.292_R_00.jp2 to VL_FI_VL00633.292_R_00.jpg\n",
      "Converted VL_FI_VL0H599.166_R_00.jp2 to VL_FI_VL0H599.166_R_00.jpg\n",
      "Converted VL_FI_VL0H628.137_R_00.jp2 to VL_FI_VL0H628.137_R_00.jpg\n",
      "Converted VL_FI_VL00963.100_R_00.jp2 to VL_FI_VL00963.100_R_00.jpg\n",
      "Converted VL_FI_VL0H313.136_R_00.jp2 to VL_FI_VL0H313.136_R_00.jpg\n",
      "Converted VL_FI_VL0H524.225_R_00.jp2 to VL_FI_VL0H524.225_R_00.jpg\n",
      "Converted VL_FI_VL00770.087_R_00.jp2 to VL_FI_VL00770.087_R_00.jpg\n",
      "Converted VL_FI_VL03639.212_R_00.jp2 to VL_FI_VL03639.212_R_00.jpg\n",
      "Converted VL_FI_VL04528.076_R_00.jp2 to VL_FI_VL04528.076_R_00.jpg\n",
      "Converted VL_FI_VL0H703.129_R_00.jp2 to VL_FI_VL0H703.129_R_00.jpg\n",
      "Converted VL_FI_VL00535.137_R_00.jp2 to VL_FI_VL00535.137_R_00.jpg\n",
      "Converted VL_FI_VL00736.199_R_00.jp2 to VL_FI_VL00736.199_R_00.jpg\n",
      "Converted VL_FI_VL00001.240_R_00.jp2 to VL_FI_VL00001.240_R_00.jpg\n",
      "Converted VL_FI_VL01474.048_R_00.jp2 to VL_FI_VL01474.048_R_00.jpg\n",
      "Converted VL_FI_VL03541.225_R_00.jp2 to VL_FI_VL03541.225_R_00.jpg\n",
      "Converted VL_FI_VLH1421.053_R_00.jp2 to VL_FI_VLH1421.053_R_00.jpg\n",
      "Converted VL_FI_VL01178.253_R_00.jp2 to VL_FI_VL01178.253_R_00.jpg\n",
      "Converted VL_FI_VL00921.207_R_00.jp2 to VL_FI_VL00921.207_R_00.jpg\n",
      "Converted VL_FI_VL04339.090_R_00.jp2 to VL_FI_VL04339.090_R_00.jpg\n",
      "Converted VL_FI_VL03988.192_R_00.jp2 to VL_FI_VL03988.192_R_00.jpg\n",
      "Converted VL_FI_VL00017.300_R_00.jp2 to VL_FI_VL00017.300_R_00.jpg\n",
      "Converted VL_FI_VL0H907.145_R_00.jp2 to VL_FI_VL0H907.145_R_00.jpg\n",
      "Converted VL_FI_VL0H703.140_R_00.jp2 to VL_FI_VL0H703.140_R_00.jpg\n",
      "Converted VL_FI_VL01003.087_R_00.jp2 to VL_FI_VL01003.087_R_00.jpg\n",
      "Converted VL_FI_VL00921.203_R_00.jp2 to VL_FI_VL00921.203_R_00.jpg\n",
      "Converted VL_FI_VL0H600.050_R_00.jp2 to VL_FI_VL0H600.050_R_00.jpg\n",
      "Converted VL_FI_VL45750.175_R_00.jp2 to VL_FI_VL45750.175_R_00.jpg\n",
      "Converted VL_FI_VL45750.174_R_00.jp2 to VL_FI_VL45750.174_R_00.jpg\n",
      "Converted VL_FI_VL0H599.164_R_00.jp2 to VL_FI_VL0H599.164_R_00.jpg\n",
      "Converted VL_FI_VL0H703.131_R_00.jp2 to VL_FI_VL0H703.131_R_00.jpg\n",
      "Converted VL_FI_VL0H628.139_R_00.jp2 to VL_FI_VL0H628.139_R_00.jpg\n",
      "Converted VL_FI_VL03541.226_R_00.jp2 to VL_FI_VL03541.226_R_00.jpg\n",
      "Converted VL_FI_VL01178.247_R_00.jp2 to VL_FI_VL01178.247_R_00.jpg\n",
      "Converted VL_FI_VL04735.060_R_00.jp2 to VL_FI_VL04735.060_R_00.jpg\n",
      "Converted VL_FI_VL04787.082_R_00.jp2 to VL_FI_VL04787.082_R_00.jpg\n",
      "Converted VL_FI_VL69400.145_R_00.jp2 to VL_FI_VL69400.145_R_00.jpg\n",
      "Converted VL_FI_VL00014.308_R_00.jp2 to VL_FI_VL00014.308_R_00.jpg\n"
     ]
    }
   ],
   "source": [
    "def open_image(image_path, convert_mode):\n",
    "    \"\"\"\n",
    "    Opens an image from the given path.\n",
    "\n",
    "    Parameters:\n",
    "    image_path: str, the path to the image.\n",
    "    convert_mode: str, the mode to convert the image to. Options are \"RGB\" and \"L\".\n",
    "\n",
    "    Returns:\n",
    "    image: Image, the opened image.\n",
    "    \"\"\"\n",
    "\n",
    "    assert convert_mode in [\"RGB\", \"L\"], \"Invalid convert mode. Options are 'RGB' and 'L'.\"\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to specified mode\n",
    "    # image = image.convert(convert_mode)\n",
    "\n",
    "    return image\n",
    "\n",
    "def display_image(image):\n",
    "    \"\"\"\n",
    "    Displays the image.\n",
    "\n",
    "    Parameters:\n",
    "    image: Image, the image to display.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    image.show()\n",
    "\n",
    "def get_palette(num_cls):\n",
    "    \"\"\" Returns the color map for visualizing the segmentation mask.\n",
    "    Args:\n",
    "        num_cls: Number of classes\n",
    "    Returns:\n",
    "        The color map\n",
    "    \"\"\"\n",
    "    n = num_cls\n",
    "    palette = [0] * (n * 3)\n",
    "    for j in range(0, n):\n",
    "        lab = j\n",
    "        palette[j * 3 + 0] = 0\n",
    "        palette[j * 3 + 1] = 0\n",
    "        palette[j * 3 + 2] = 0\n",
    "        i = 0\n",
    "        while lab:\n",
    "            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
    "            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
    "            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
    "            i += 1\n",
    "            lab >>= 3\n",
    "    return palette\n",
    "\n",
    "\n",
    "def resize_images_in_folder(source_folder, destination_folder, size=(336, 336)):\n",
    "    \"\"\"\n",
    "    Iterates through a given folder, opens each image file, resizes it to the specified size,\n",
    "    and saves it to the specified destination folder.\n",
    "\n",
    "    Parameters:\n",
    "    - source_folder: str, the folder containing the images to resize.\n",
    "    - destination_folder: str, the folder to save the resized images.\n",
    "    - size: tuple, the target size for resizing (default is (336, 336)).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    # Ensure the destination folder exists\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    # Iterate through each file in the source folder\n",
    "    for filename in os.listdir(source_folder):\n",
    "        file_path = os.path.join(source_folder, filename)\n",
    "        \n",
    "        # Check if the file is an image\n",
    "        try:\n",
    "            with Image.open(file_path) as img:\n",
    "                # Resize the image\n",
    "                img_resized = img.resize(size, Image.ANTIALIAS)\n",
    "                \n",
    "                # Save the resized image to the destination folder\n",
    "                save_path = os.path.join(destination_folder, filename)\n",
    "                img_resized.save(save_path)\n",
    "                print(f\"Resized and saved {filename} to {save_path}\")\n",
    "        except IOError:\n",
    "            print(f\"Skipping non-image file: {filename}\")\n",
    "\n",
    "\n",
    "def convert_jp2_to_jpg(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jp2'):\n",
    "            # Open the jp2 file\n",
    "            jp2_path = os.path.join(folder_path, filename)\n",
    "            img = Image.open(jp2_path)\n",
    "            \n",
    "            # Convert the filename to .jpg\n",
    "            new_filename = os.path.splitext(filename)[0] + '.jpg'\n",
    "            jpg_path = os.path.join(folder_path, new_filename)\n",
    "            \n",
    "            # Save the image as a jpg\n",
    "            img.convert('RGB').save(jpg_path, 'JPEG')\n",
    "            \n",
    "            # Optionally, delete the original .jp2 file\n",
    "            # os.remove(jp2_path)\n",
    "            \n",
    "            print(f\"Converted {filename} to {new_filename}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "folder_path = '../data/336'\n",
    "convert_jp2_to_jpg(folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encoder(image, model, transform, save_folder, filename):\n",
    "    \"\"\"\n",
    "    Use CLIP model to encode the image and save the tranformed version.\n",
    "    First resizes the image to 224x224, then normalizes it, and finally encodes it.\n",
    "    Then, encodes the image into a 512-dimensional feature vector.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to encode.\n",
    "    - model: CLIP model, the model used for encoding.\n",
    "    - transform: CLIP transform, the transformation to required for CLIP.\n",
    "    - save_folder: str, the folder to save the transformed image.\n",
    "    - filename: str, the name of the file to save the transformed image as.\n",
    "\n",
    "    Returns:\n",
    "    - image_features: torch.Tensor, the encoded image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CLIP model\n",
    "    model = model.eval().to(DEVICE)\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Encode the image\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "\n",
    "    # # Ensure the save folder exists\n",
    "    # if not os.path.exists(save_folder):\n",
    "    #     print(f\"Creating folder {save_folder}...\")\n",
    "    #     os.makedirs(save_folder)\n",
    "    \n",
    "    # # Save the transformed image\n",
    "    # save_path = os.path.join(save_folder, filename)\n",
    "    # transformed_image_pil = transforms.ToPILImage()(image.squeeze(0).cpu())\n",
    "    # transformed_image_pil.save(save_path)\n",
    "\n",
    "    return image_features\n",
    "\n",
    "def create_reference_embeddings(source_dir, CLIP_model, CLIP_transform, convert_mode, save_folder):\n",
    "    \"\"\"\n",
    "    Creates the image embeddings for the images in the source directory and saves together with labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir: str, the path to the directory containing the images.\n",
    "    - CLIP_model: CLIP model, the CLIP model to use for encoding.\n",
    "    - CLIP_transform: CLIP transforms, the CLIP transformation to apply to the images.\n",
    "    - convert_mode: str, the mode to convert the image to. Options are \"RGB\" and \"L\".\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of files in the source directory\n",
    "    sub_files = os.listdir(source_dir)\n",
    "\n",
    "    # Initialize the list of image features and labels\n",
    "    design_features_list = []\n",
    "    design_labels_list = []\n",
    "\n",
    "    for file in sub_files:\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "        print(f\"Processing {file}...\")\n",
    "\n",
    "        # Get the path to the folder containing the images\n",
    "        image_path = os.path.join(source_dir, file)\n",
    "\n",
    "        # Load the images from the folder\n",
    "        image = open_image(image_path, convert_mode)\n",
    "\n",
    "        # Embed the image\n",
    "        image_features = image_encoder(image, CLIP_model, CLIP_transform, save_folder, filename=file)\n",
    "\n",
    "        # Append the image features and labels to the lists\n",
    "        design_features_list.append(image_features)\n",
    "        design_labels_list.append(file)\n",
    "\n",
    "    # Save the image features and labels\n",
    "    with open(f'../data/design_embeddings_{convert_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(design_features_list, f)\n",
    "    with open(f'../data/design_labels_{convert_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(design_labels_list, f)\n",
    "\n",
    "def get_segmentation_mask(image, processor, model):\n",
    "    \"\"\"\n",
    "    Function to segment clothes in an image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to segment.\n",
    "    - processor: SegformerImageProcessor object, the processor used to preprocess the image.\n",
    "    - model: AutoModelForSemanticSegmentation object, the model used to segment the image.\n",
    "\n",
    "    Returns:\n",
    "    - pred_seg: torch.Tensor, the segmented image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "    # Create a mask for the labels 4, 5, 6, and 7\n",
    "    mask = (pred_seg == 4) | (pred_seg == 5) | (pred_seg == 6) | (pred_seg == 7) | (pred_seg == 8) | (pred_seg == 16) | (pred_seg == 17)\n",
    "\n",
    "    # Set all other labels to 0\n",
    "    pred_seg[~mask] = 0\n",
    "\n",
    "    # Set the labels 4, 5, 6, and 7 to 255\n",
    "    pred_seg[mask] = 255\n",
    "\n",
    "    return pred_seg\n",
    "    \n",
    "    # plt.imshow(pred_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    \"\"\"\n",
    "    Converts a PIL image to a tensor.\n",
    "\n",
    "    Parameters:\n",
    "    image: PIL Image, the image to convert.\n",
    "\n",
    "    Returns:\n",
    "    tensor: Tensor, the converted tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts a tensor to a PIL image.\n",
    "\n",
    "    Parameters:\n",
    "    tensor: Tensor, the tensor image to convert.\n",
    "\n",
    "    Returns:\n",
    "    image: PIL Image, the converted image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the tensor to an image\n",
    "    image = transforms.ToPILImage()(tensor)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def apply_random_rotation(image_tensor, degrees=30):\n",
    "    \"\"\"\n",
    "    Applies random rotation to the given tensor image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    degrees: int or tuple, range of degrees to select from.\n",
    "\n",
    "    Returns:\n",
    "    rotated_tensor: Tensor, the image tensor with random rotation applied.\n",
    "    \"\"\"\n",
    "    # Create a RandomRotation transform\n",
    "    random_rotation = transforms.RandomRotation(degrees=degrees)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    rotated_tensor = random_rotation(image_tensor)\n",
    "\n",
    "    return rotated_tensor\n",
    "\n",
    "\n",
    "def downsample_and_upsample(image_tensor, downsample_level=5):\n",
    "    \"\"\"\n",
    "    Downsamples an input tensor to a specified level and then upsamples it to the original size.\n",
    "\n",
    "    A proper range for downsample_level is 5 to 10.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    downsample_level: int, the factor by which to downsample.\n",
    "\n",
    "    Returns:\n",
    "    upsampled_tensor: Tensor, the upsampled image tensor.\n",
    "    \"\"\"\n",
    "    # Get the original size of the image tensor\n",
    "    original_size = image_tensor.shape[-2:]\n",
    "\n",
    "    # Calculate the downsampled size\n",
    "    downsampled_size = (original_size[0] // downsample_level, original_size[1] // downsample_level)\n",
    "\n",
    "    # Downsample the image tensor\n",
    "    downsampled_tensor = F.interpolate(image_tensor.unsqueeze(0), size=downsampled_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    # Upsample the image tensor back to the original size\n",
    "    upsampled_tensor = F.interpolate(downsampled_tensor.unsqueeze(0), size=original_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    return upsampled_tensor\n",
    "\n",
    "\n",
    "# def gaussian_blur(image_tensor, kernel_size=5, sigma=1.0):\n",
    "#     \"\"\"\n",
    "#     Applies a Gaussian blur to a given tensor.\n",
    "\n",
    "#     Parameters:\n",
    "#     image_tensor: Tensor, the input image tensor.\n",
    "#     kernel_size: int, the size of the Gaussian kernel.\n",
    "#     sigma: float, the standard deviation of the Gaussian kernel.\n",
    "\n",
    "#     Returns:\n",
    "#     blurred_tensor: Tensor, the blurred image tensor.\n",
    "#     \"\"\"\n",
    "#     # Define the Gaussian blur transform\n",
    "#     gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
    "\n",
    "#     # Apply the Gaussian blur to the image tensor\n",
    "#     blurred_tensor = gaussian_blur(image_tensor)\n",
    "\n",
    "#     return blurred_tensor\n",
    "\n",
    "\n",
    "def random_jpeg_compression(image_tensor, min_quality=30, max_quality=70):\n",
    "    \"\"\"\n",
    "    Applies random JPEG compression with varying levels of quality to simulate artifacts and lower quality in images.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    min_quality: int, the minimum JPEG quality.\n",
    "    max_quality: int, the maximum JPEG quality.\n",
    "\n",
    "    Returns:\n",
    "    compressed_tensor: Tensor, the compressed image tensor.\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "\n",
    "    # Generate a random quality level between min_quality and max_quality\n",
    "    quality = random.randint(min_quality, max_quality)\n",
    "\n",
    "    # Save the PIL image to a bytes buffer with the generated quality level\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format='JPEG', quality=quality)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Load the image back from the bytes buffer\n",
    "    compressed_image = Image.open(buffer)\n",
    "\n",
    "    # Convert the PIL image back to a tensor\n",
    "    compressed_tensor = transforms.ToTensor()(compressed_image)\n",
    "\n",
    "    return compressed_tensor\n",
    "\n",
    "\n",
    "def random_mask(image_tensor, mask_size=500, area_to_mask=4000000):\n",
    "    \"\"\"\n",
    "    Randomly masks out regions of the image tensor.\n",
    "\n",
    "    Proper range for mask_size is 500 to 1000.\n",
    "    and for num_masks is such that in total 4k pixels are masked.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    mask_size: int, the size of the mask.\n",
    "    num_masks: int, the number of masks to apply.\n",
    "\n",
    "    Returns:\n",
    "    masked_tensor: Tensor, the masked image tensor.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the image tensor\n",
    "    _, height, width = image_tensor.shape\n",
    "\n",
    "    # Create a copy of the image tensor to apply masks\n",
    "    masked_tensor = image_tensor.clone()\n",
    "\n",
    "    # Calculate the area of one mask\n",
    "    mask_area = mask_size * mask_size\n",
    "\n",
    "    # Calculate the number of masks needed defaulting to 4M pixels\n",
    "    num_masks = area_to_mask // mask_area\n",
    "\n",
    "    print(f\"Number of masks: {num_masks}\")\n",
    "\n",
    "    for _ in range(num_masks):\n",
    "        # Randomly select the top-left corner of the mask\n",
    "        top = random.randint(0, height - mask_size)\n",
    "        left = random.randint(0, width - mask_size)\n",
    "\n",
    "        # Apply the mask by setting the selected region to zero\n",
    "        masked_tensor[:, top:top + mask_size, left:left + mask_size] = 0\n",
    "\n",
    "    return masked_tensor\n",
    "\n",
    "\n",
    "def random_color_jitter(image_tensor, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1):\n",
    "    \"\"\"\n",
    "    Applies random color jitter to the given tensor image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    brightness: float or tuple, how much to jitter brightness.\n",
    "    contrast: float or tuple, how much to jitter contrast.\n",
    "    saturation: float or tuple, how much to jitter saturation.\n",
    "    hue: float or tuple, how much to jitter hue.\n",
    "\n",
    "    Returns:\n",
    "    jittered_tensor: Tensor, the image tensor with random color jitter applied.\n",
    "    \"\"\"\n",
    "    # Create a ColorJitter transform\n",
    "    color_jitter = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    jittered_tensor = color_jitter(image_tensor)\n",
    "\n",
    "    return jittered_tensor\n",
    "\n",
    "\n",
    "def add_synthetic_shadows(image_tensor, num_shadows=3, shadow_intensity=0.5, shadow_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Adds synthetic shadows to an image tensor to mimic uneven lighting conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_tensor (torch.Tensor): The input image tensor with shape (C, H, W).\n",
    "    - num_shadows (int): Number of shadow shapes to add.\n",
    "    - shadow_intensity (float): The intensity of the shadows (0 = no shadow, 1 = completely black).\n",
    "    - shadow_color (tuple): The color of the shadow in RGB.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The image tensor with synthetic shadows.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, H, W = image_tensor.shape\n",
    "    shadow_image = image_tensor.clone()\n",
    "\n",
    "    for _ in range(num_shadows):\n",
    "        # Randomly generate an ellipse\n",
    "        center_x = np.random.randint(0, W)\n",
    "        center_y = np.random.randint(0, H)\n",
    "        axis_x = np.random.randint(W // 8, W // 2)\n",
    "        axis_y = np.random.randint(H // 8, H // 2)\n",
    "        angle = np.random.uniform(0, 180)\n",
    "        angle = torch.tensor(angle)  # Convert angle to a tensor\n",
    "\n",
    "\n",
    "        # Create a meshgrid for the image\n",
    "        Y, X = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "\n",
    "        # Apply the ellipse equation\n",
    "        ellipse = (((X - center_x) * torch.cos(angle) + (Y - center_y) * torch.sin(angle)) ** 2) / axis_x ** 2 + \\\n",
    "                  (((X - center_x) * torch.sin(angle) - (Y - center_y) * torch.cos(angle)) ** 2) / axis_y ** 2\n",
    "\n",
    "        # Create a mask where the ellipse condition is satisfied\n",
    "        mask = ellipse <= 1\n",
    "\n",
    "        # Apply the shadow by reducing the intensity of the masked region\n",
    "        for i in range(3):  # Assuming image is RGB\n",
    "            shadow_image[i][mask] = (shadow_image[i][mask] * (1 - shadow_intensity) + \n",
    "                                      shadow_color[i] * shadow_intensity)\n",
    "\n",
    "    return shadow_image\n",
    "\n",
    "\n",
    "def apply_random_shearing(image_tensor, shear):\n",
    "    \"\"\"\n",
    "    Applies random shearing effect to the given tensor image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    shear: float or tuple, range of degrees to select from for shearing.\n",
    "\n",
    "    Returns:\n",
    "    sheared_tensor: Tensor, the image tensor with random shearing applied.\n",
    "    \"\"\"\n",
    "    # Create a RandomAffine transform with shearing\n",
    "    random_shearing = transforms.RandomAffine(degrees=0, shear=shear)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    sheared_tensor = random_shearing(image_tensor)\n",
    "\n",
    "    return sheared_tensor\n",
    "\n",
    "\n",
    "def apply_perspective_transform(image_tensor, distortion_scale=0.5, p=1.0):\n",
    "    \"\"\"\n",
    "    Applies perspective transformations to simulate viewing the image from different angles.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    distortion_scale: float, the degree of distortion (0 to 1).\n",
    "    p: float, probability of applying the transformation.\n",
    "\n",
    "    Returns:\n",
    "    transformed_tensor: Tensor, the image tensor with perspective transformation applied.\n",
    "    \"\"\"\n",
    "    # Create a RandomPerspective transform\n",
    "    perspective_transform = transforms.RandomPerspective(distortion_scale=distortion_scale, p=p)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    transformed_tensor = perspective_transform(image_tensor)\n",
    "\n",
    "    return transformed_tensor\n",
    "\n",
    "\n",
    "def apply_photographic_transformations(image_tensor, gamma_range=(0.8, 1.2), exposure_range=(0.8, 1.2), lighting_direction_range=(0.8, 1.2)):\n",
    "    \"\"\"\n",
    "    Applies transformations like random changes in gamma, exposure, or lighting direction to simulate different photographic conditions.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    gamma_range: tuple, range of gamma values to select from.\n",
    "    exposure_range: tuple, range of exposure values to select from.\n",
    "    lighting_direction_range: tuple, range of lighting direction values to select from.\n",
    "\n",
    "    Returns:\n",
    "    transformed_tensor: Tensor, the image tensor with photographic transformations applied.\n",
    "    \"\"\"\n",
    "    # Apply random gamma correction\n",
    "    gamma = random.uniform(*gamma_range)\n",
    "    gamma_transform = transforms.functional.adjust_gamma(image_tensor, gamma)\n",
    "    \n",
    "    # Apply random exposure adjustment\n",
    "    exposure = random.uniform(*exposure_range)\n",
    "    exposure_transform = transforms.functional.adjust_brightness(gamma_transform, exposure)\n",
    "    \n",
    "    # Apply random lighting direction adjustment (simulated using brightness and contrast)\n",
    "    lighting_direction = random.uniform(*lighting_direction_range)\n",
    "    lighting_transform = transforms.functional.adjust_contrast(exposure_transform, lighting_direction)\n",
    "    \n",
    "    return lighting_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load an image and convert to tensor\n",
    "# image = Image.open('/Users/ilerisoy/Library/CloudStorage/GoogleDrive-mtilerisoy@gmail.com/My Drive/Vlisco/ML-based-Image-Matching/data/designs/VL00815.jpg')\n",
    "# image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# # Apply photographic transformations to the image tensor\n",
    "# transformed_tensor = apply_photographic_transformations(image_tensor, gamma_range=(0.8, 1.2), exposure_range=(0.8, 1.2), lighting_direction_range=(0.8, 1.2))\n",
    "\n",
    "# # Convert back to PIL image to visualize\n",
    "# transformed_image = transforms.ToPILImage()(transformed_tensor)\n",
    "# transformed_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VL0H703_131.jp2...\n",
      "Processing VL48350_031.jp2...\n",
      "Processing VL0H522_093.jp2...\n",
      "Processing VL00014_308.jp2...\n",
      "Processing VL50950_176.jp2...\n",
      "Processing VL04339_090.jp2...\n",
      "Processing VL76950_202.jp2...\n",
      "Processing VL0H876_078.jp2...\n",
      "Processing VLH1498_020.jp2...\n",
      "Processing VL03999_086.jp2...\n",
      "Processing VL03639_212.jp2...\n",
      "Processing VL03636_136.jp2...\n",
      "Processing VL65450_048.jp2...\n",
      "Processing VL01681_063.jp2...\n",
      "Processing VL04918_075.jp2...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m seg_model \u001b[38;5;241m=\u001b[39m AutoModelForSemanticSegmentation\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmattmdjaga/segformer_b2_clothes\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create the reference embeddings\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mcreate_reference_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesigns_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Load the design database embeddings and labels\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/design_embeddings_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconvert_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "Cell \u001b[0;32mIn[31], line 75\u001b[0m, in \u001b[0;36mcreate_reference_embeddings\u001b[0;34m(source_dir, CLIP_model, CLIP_transform, convert_mode, save_folder)\u001b[0m\n\u001b[1;32m     72\u001b[0m image \u001b[38;5;241m=\u001b[39m open_image(image_path, convert_mode)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Embed the image\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[43mimage_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIP_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# Append the image features and labels to the lists\u001b[39;00m\n\u001b[1;32m     78\u001b[0m design_features_list\u001b[38;5;241m.\u001b[39mappend(image_features)\n",
      "Cell \u001b[0;32mIn[31], line 23\u001b[0m, in \u001b[0;36mimage_encoder\u001b[0;34m(image, model, transform, save_folder, filename)\u001b[0m\n\u001b[1;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Preprocess the image\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Encode the image\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mantialias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torchvision/transforms/functional.py:468\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    466\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    467\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_pil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_interpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/PIL/Image.py:2185\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   2183\u001b[0m size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(size)\n\u001b[0;32m-> 2185\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m box \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2187\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/PIL/Jpeg2KImagePlugin.py:313\u001b[0m, in \u001b[0;36mJpeg2KImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    310\u001b[0m     t3 \u001b[38;5;241m=\u001b[39m (t[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers, t[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m3\u001b[39m], t[\u001b[38;5;241m3\u001b[39m][\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile \u001b[38;5;241m=\u001b[39m [(t[\u001b[38;5;241m0\u001b[39m], (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, t[\u001b[38;5;241m2\u001b[39m], t3)]\n\u001b[0;32m--> 313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/pg/lib/python3.8/site-packages/PIL/ImageFile.py:266\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m decoder\u001b[38;5;241m.\u001b[39mpulls_fd:\n\u001b[1;32m    265\u001b[0m     decoder\u001b[38;5;241m.\u001b[39msetfd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp)\n\u001b[0;32m--> 266\u001b[0m     err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    268\u001b[0m     b \u001b[38;5;241m=\u001b[39m prefix\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DEVICE = \"mps\"\n",
    "\n",
    "# Source directory containing the scraped folders\n",
    "# designs_dir = \"../data/336\"\n",
    "designs_dir = \"/Users/ilerisoy/Downloads/Classics/designs\"\n",
    "\n",
    "convert_mode = \"RGB\"\n",
    "\n",
    "# Load the CLIP model\n",
    "CLIP_model, CLIP_transform = clip.load(\"ViT-L/14@336px\")\n",
    "\n",
    "# Segmentation model initialization\n",
    "seg_processor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "seg_model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "\n",
    "# Create the reference embeddings\n",
    "create_reference_embeddings(designs_dir, CLIP_model, CLIP_transform, convert_mode=convert_mode, save_folder=\"\")\n",
    "\n",
    "# Load the design database embeddings and labels\n",
    "with open(f'../data/design_embeddings_{convert_mode}.pkl', 'rb') as f:\n",
    "    design_embeddings = pickle.load(f)\n",
    "with open(f'../data/design_labels_{convert_mode}.pkl', 'rb') as f:\n",
    "    design_labels = pickle.load(f)\n",
    "\n",
    "print(f'Total number of embeddings: {len(design_embeddings)}')\n",
    "print(f'Type of design embeddings: {type(design_embeddings)}')\n",
    "print(f'Design Labels: {design_labels}')\n",
    "print(f'Length of Design Labels: {len(design_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create triplets\n",
    "\n",
    "It will take apprx. 1 hour to transform 250 designs -> 12 sec per design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VL0H516.jpg...\n",
      "Time taken: 0.3835258483886719\n",
      "Top K similarity values: tensor([0.7664, 0.7593, 0.7535, 0.7511, 0.7481], device='mps:0')\n",
      "Top K design labels: ['VL04490.jpg', 'VL48350.jpg', 'VL00633.jpg', 'VL01201.jpg', 'VL73650.jpg']\n"
     ]
    }
   ],
   "source": [
    "def create_triplets(source_dir):\n",
    "    \n",
    "    # List all files in the source directory\n",
    "    sub_files = os.listdir(source_dir)\n",
    "\n",
    "    for file in sub_files:\n",
    "        if file == \".DS_Store\" or file == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        print(f\"Processing {file}...\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Get the path to the folder containing the images\n",
    "        image_path = os.path.join(source_dir, file)\n",
    "\n",
    "        # Load the images from the folder\n",
    "        image = open_image(image_path, convert_mode=convert_mode)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image_tensor = image_to_tensor(image)\n",
    "\n",
    "        # # Apply random rotation to the image tensor\n",
    "        # rotated_tensor = apply_random_rotation(image_tensor, degrees=30)\n",
    "\n",
    "        # # Apply downsample and upsample to the image tensor\n",
    "        # downsampled_upsampled_tensor = downsample_and_upsample(image_tensor, downsample_level=10)\n",
    "\n",
    "        # # Apply random JPEG compression to the image tensor\n",
    "        # compressed_tensor = random_jpeg_compression(image_tensor, min_quality=30, max_quality=50)\n",
    "\n",
    "        # # Apply random masks to the image tensor\n",
    "        # masked_tensor = random_mask(image_tensor, mask_size=900)\n",
    "\n",
    "        # # Apply color jitter to the image tensor\n",
    "        # jittered_tensor = random_color_jitter(image_tensor, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
    "\n",
    "        # # Add synthetic shadows to the image tensor\n",
    "        # shadowed_tensor = add_synthetic_shadows(image_tensor)\n",
    "\n",
    "        # # Apply perspective transformation to the image tensor\n",
    "        # transformed_tensor = apply_perspective_transform(image_tensor, distortion_scale=0.5)\n",
    "\n",
    "        # # Apply random shearing to the image tensor\n",
    "        # sheared_tensor = apply_random_shearing(image_tensor, shear=20)\n",
    "\n",
    "        # # Apply photographic transformations to the image tensor\n",
    "        # photographic_tensor = apply_photographic_transformations(image_tensor, gamma_range=(0.8, 1.2), exposure_range=(0.8, 1.2), lighting_direction_range=(0.8, 1.2))\n",
    "\n",
    "        print(f\"Time taken: {time.time() - start}\")\n",
    "\n",
    "        # Embed the image\n",
    "        image_features = image_encoder(image, CLIP_model, CLIP_transform)\n",
    "\n",
    "        # Do cosine similarity with the design embeddings\n",
    "        similarities = [torch.nn.functional.cosine_similarity(image_features, t) for t in design_embeddings]\n",
    "        similarities = torch.stack(similarities)\n",
    "        \n",
    "        # Get the index of the most k similar designs\n",
    "        k = 10\n",
    "        top_k_similarities = similarities.T.topk(k)\n",
    "\n",
    "        # Get the design labels of the top k similar designs\n",
    "        top_k_design_labels = [design_labels[i] for i in top_k_similarities.indices[0]]\n",
    "\n",
    "        print(f\"Top K similarity values: {top_k_similarities.values[0,-5:]}\")\n",
    "        print(f\"Top K design labels: {top_k_design_labels[-5:]}\")\n",
    "        \n",
    "        break\n",
    "    \n",
    "\n",
    "create_triplets(designs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######## in VL0H703_131.jp2   || Top K similarity values: tensor([[0.7593, 0.7538, 0.7457, 0.7443, 0.7420, 0.7361, 0.7308, 0.7305, 0.7270,\n",
      "         0.7261]], device='mps:0')\n",
      "######## in VL0H522_093.jp2   || Top K similarity values: tensor([[0.7463, 0.7322, 0.7311, 0.7299, 0.7249, 0.7165, 0.7153, 0.7152, 0.7145,\n",
      "         0.7129]], device='mps:0')\n",
      "MATCH: 1 in VL00014_308.jp2   || Top K similarity values: tensor([[0.7771, 0.7635, 0.7620, 0.7594, 0.7542, 0.7502, 0.7469, 0.7454, 0.7444,\n",
      "         0.7417]], device='mps:0')\n",
      "######## in VL50950_176.jp2   || Top K similarity values: tensor([[0.7138, 0.7023, 0.7009, 0.7008, 0.6901, 0.6893, 0.6881, 0.6879, 0.6826,\n",
      "         0.6787]], device='mps:0')\n",
      "MATCH: 2 in VL04339_090.jp2   || Top K similarity values: tensor([[0.7299, 0.7211, 0.7104, 0.7085, 0.7067, 0.7061, 0.6961, 0.6960, 0.6944,\n",
      "         0.6908]], device='mps:0')\n",
      "MATCH: 3 in VL76950_202.jp2   || Top K similarity values: tensor([[0.7798, 0.7643, 0.7627, 0.7625, 0.7580, 0.7570, 0.7545, 0.7534, 0.7533,\n",
      "         0.7502]], device='mps:0')\n",
      "######## in VL0H876_078.jp2   || Top K similarity values: tensor([[0.7820, 0.7705, 0.7629, 0.7626, 0.7576, 0.7520, 0.7514, 0.7514, 0.7491,\n",
      "         0.7462]], device='mps:0')\n",
      "MATCH: 4 in VLH1498_020.jp2   || Top K similarity values: tensor([[0.7627, 0.7466, 0.7401, 0.7347, 0.7309, 0.7287, 0.7241, 0.7183, 0.7134,\n",
      "         0.7117]], device='mps:0')\n",
      "######## in VL03999_086.jp2   || Top K similarity values: tensor([[0.7487, 0.7446, 0.7366, 0.7362, 0.7360, 0.7352, 0.7351, 0.7335, 0.7325,\n",
      "         0.7264]], device='mps:0')\n",
      "MATCH: 5 in VL03639_212.jp2   || Top K similarity values: tensor([[0.7754, 0.7620, 0.7616, 0.7610, 0.7594, 0.7522, 0.7499, 0.7492, 0.7486,\n",
      "         0.7484]], device='mps:0')\n",
      "MATCH: 6 in VL03636_136.jp2   || Top K similarity values: tensor([[0.8060, 0.8057, 0.8045, 0.8030, 0.7986, 0.7960, 0.7908, 0.7907, 0.7903,\n",
      "         0.7892]], device='mps:0')\n",
      "MATCH: 7 in VL65450_048.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL01681_063.jp2   || Top K similarity values: tensor([[0.6785, 0.6715, 0.6668, 0.6623, 0.6604, 0.6586, 0.6541, 0.6523, 0.6518,\n",
      "         0.6515]], device='mps:0')\n",
      "MATCH: 8 in VL04918_075.jp2   || Top K similarity values: tensor([[0.7788, 0.7630, 0.7529, 0.7467, 0.7464, 0.7427, 0.7348, 0.7312, 0.7308,\n",
      "         0.7300]], device='mps:0')\n",
      "######## in VL04787_082.jp2   || Top K similarity values: tensor([[0.8050, 0.7924, 0.7861, 0.7670, 0.7641, 0.7626, 0.7622, 0.7610, 0.7607,\n",
      "         0.7602]], device='mps:0')\n",
      "MATCH: 9 in VL0H596_111.jp2   || Top K similarity values: tensor([[0.7350, 0.7347, 0.7187, 0.7173, 0.7170, 0.7094, 0.7042, 0.7036, 0.6996,\n",
      "         0.6989]], device='mps:0')\n",
      "MATCH: 10 in VL03327_129.jp2   || Top K similarity values: tensor([[0.7451, 0.7350, 0.7349, 0.7252, 0.7201, 0.7194, 0.7179, 0.7165, 0.7133,\n",
      "         0.7133]], device='mps:0')\n",
      "MATCH: 11 in VL01003_124.jp2   || Top K similarity values: tensor([[0.7770, 0.7636, 0.7520, 0.7441, 0.7417, 0.7383, 0.7358, 0.7253, 0.7157,\n",
      "         0.7152]], device='mps:0')\n",
      "######## in VL00736_199.jp2   || Top K similarity values: tensor([[0.7494, 0.7326, 0.7317, 0.7305, 0.7304, 0.7284, 0.7272, 0.7262, 0.7245,\n",
      "         0.7235]], device='mps:0')\n",
      "MATCH: 12 in VL00535_145.jp2   || Top K similarity values: tensor([[0.7418, 0.7321, 0.7299, 0.7231, 0.7109, 0.7064, 0.7033, 0.7031, 0.7021,\n",
      "         0.7011]], device='mps:0')\n",
      "MATCH: 13 in VL0H600_050.jp2   || Top K similarity values: tensor([[0.6880, 0.6848, 0.6845, 0.6797, 0.6783, 0.6762, 0.6752, 0.6724, 0.6705,\n",
      "         0.6703]], device='mps:0')\n",
      "MATCH: 14 in VL00017_342.jp2   || Top K similarity values: tensor([[0.7580, 0.7561, 0.7349, 0.7306, 0.7265, 0.7212, 0.7200, 0.7189, 0.7180,\n",
      "         0.7180]], device='mps:0')\n",
      "MATCH: 15 in VL00052_153.jp2   || Top K similarity values: tensor([[0.7722, 0.7707, 0.7693, 0.7676, 0.7665, 0.7602, 0.7588, 0.7501, 0.7491,\n",
      "         0.7491]], device='mps:0')\n",
      "MATCH: 16 in VL00864_001.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "MATCH: 17 in VL01003_087.jp2   || Top K similarity values: tensor([[0.7724, 0.7519, 0.7292, 0.7291, 0.7286, 0.7222, 0.7212, 0.7175, 0.7163,\n",
      "         0.7129]], device='mps:0')\n",
      "MATCH: 18 in VL03143_077.jp2   || Top K similarity values: tensor([[0.7514, 0.7493, 0.7383, 0.7382, 0.7365, 0.7360, 0.7349, 0.7309, 0.7275,\n",
      "         0.7275]], device='mps:0')\n",
      "######## in VL02316_117.jp2   || Top K similarity values: tensor([[0.7294, 0.7280, 0.7266, 0.7227, 0.7222, 0.7210, 0.7203, 0.7168, 0.7138,\n",
      "         0.7130]], device='mps:0')\n",
      "MATCH: 19 in VL03499_055.jp2   || Top K similarity values: tensor([[0.7377, 0.7256, 0.7252, 0.7244, 0.7167, 0.7162, 0.7127, 0.7103, 0.7092,\n",
      "         0.7084]], device='mps:0')\n",
      "######## in VLH1003_135.jp2   || Top K similarity values: tensor([[0.7545, 0.7453, 0.7422, 0.7392, 0.7383, 0.7274, 0.7250, 0.7237, 0.7220,\n",
      "         0.7203]], device='mps:0')\n",
      "MATCH: 20 in VL00017_346.jp2   || Top K similarity values: tensor([[0.7739, 0.7510, 0.7458, 0.7412, 0.7389, 0.7306, 0.7293, 0.7287, 0.7269,\n",
      "         0.7248]], device='mps:0')\n",
      "MATCH: 21 in VL00924_247.jp2   || Top K similarity values: tensor([[0.7731, 0.7617, 0.7608, 0.7561, 0.7552, 0.7523, 0.7499, 0.7454, 0.7440,\n",
      "         0.7415]], device='mps:0')\n",
      "MATCH: 22 in VL00511_279.jp2   || Top K similarity values: tensor([[0.7453, 0.7421, 0.7329, 0.7291, 0.7271, 0.7250, 0.7211, 0.7185, 0.7177,\n",
      "         0.7150]], device='mps:0')\n",
      "MATCH: 23 in VL00921_260.jp2   || Top K similarity values: tensor([[0.7416, 0.7320, 0.7251, 0.7198, 0.7113, 0.7095, 0.7093, 0.7074, 0.7072,\n",
      "         0.7069]], device='mps:0')\n",
      "MATCH: 24 in VL03988_252.jp2   || Top K similarity values: tensor([[0.7531, 0.7251, 0.7195, 0.7124, 0.7112, 0.7086, 0.7055, 0.6979, 0.6974,\n",
      "         0.6971]], device='mps:0')\n",
      "MATCH: 25 in VL03854_067.jp2   || Top K similarity values: tensor([[0.7594, 0.7556, 0.7510, 0.7472, 0.7469, 0.7463, 0.7412, 0.7389, 0.7378,\n",
      "         0.7348]], device='mps:0')\n",
      "######## in VL49600_061.jp2   || Top K similarity values: tensor([[0.7263, 0.7194, 0.7060, 0.7058, 0.7022, 0.7020, 0.7018, 0.7011, 0.7007,\n",
      "         0.6985]], device='mps:0')\n",
      "MATCH: 26 in VL08863_083.jp2   || Top K similarity values: tensor([[0.7025, 0.6989, 0.6965, 0.6937, 0.6925, 0.6895, 0.6867, 0.6853, 0.6853,\n",
      "         0.6847]], device='mps:0')\n",
      "MATCH: 27 in VL00633_292.jp2   || Top K similarity values: tensor([[0.7782, 0.7590, 0.7551, 0.7526, 0.7514, 0.7498, 0.7489, 0.7487, 0.7483,\n",
      "         0.7457]], device='mps:0')\n",
      "MATCH: 28 in VL04528_083.jp2   || Top K similarity values: tensor([[0.7254, 0.7175, 0.7136, 0.7124, 0.7015, 0.6970, 0.6965, 0.6952, 0.6921,\n",
      "         0.6884]], device='mps:0')\n",
      "######## in VL0H638_016.jp2   || Top K similarity values: tensor([[0.7292, 0.7111, 0.7110, 0.7067, 0.7035, 0.7006, 0.6995, 0.6986, 0.6973,\n",
      "         0.6970]], device='mps:0')\n",
      "######## in VLH1167_104.jp2   || Top K similarity values: tensor([[0.7474, 0.7371, 0.7349, 0.7318, 0.7305, 0.7296, 0.7278, 0.7232, 0.7211,\n",
      "         0.7196]], device='mps:0')\n",
      "######## in VL00034_202.jp2   || Top K similarity values: tensor([[0.7454, 0.7392, 0.7378, 0.7360, 0.7356, 0.7345, 0.7307, 0.7292, 0.7261,\n",
      "         0.7240]], device='mps:0')\n",
      "MATCH: 29 in VL0H418_012.jp2   || Top K similarity values: tensor([[0.7385, 0.7169, 0.7097, 0.7091, 0.7067, 0.7043, 0.7041, 0.7020, 0.7007,\n",
      "         0.7004]], device='mps:0')\n",
      "MATCH: 30 in VL01451_068.jp2   || Top K similarity values: tensor([[0.7697, 0.7547, 0.7525, 0.7506, 0.7501, 0.7495, 0.7492, 0.7484, 0.7472,\n",
      "         0.7466]], device='mps:0')\n",
      "MATCH: 31 in VL01681_071.jp2   || Top K similarity values: tensor([[0.7211, 0.7130, 0.7125, 0.7083, 0.7047, 0.6994, 0.6963, 0.6963, 0.6960,\n",
      "         0.6910]], device='mps:0')\n",
      "######## in VL03713_044.jp2   || Top K similarity values: tensor([[0.7743, 0.7696, 0.7552, 0.7520, 0.7514, 0.7493, 0.7484, 0.7467, 0.7446,\n",
      "         0.7436]], device='mps:0')\n",
      "MATCH: 32 in VL02511_224.jp2   || Top K similarity values: tensor([[0.7790, 0.7529, 0.7512, 0.7497, 0.7460, 0.7450, 0.7446, 0.7359, 0.7341,\n",
      "         0.7286]], device='mps:0')\n",
      "######## in VL00963_150.jp2   || Top K similarity values: tensor([[0.7863, 0.7727, 0.7692, 0.7648, 0.7621, 0.7599, 0.7546, 0.7546, 0.7530,\n",
      "         0.7528]], device='mps:0')\n",
      "######## in VL00768_115.jp2   || Top K similarity values: tensor([[0.7694, 0.7548, 0.7529, 0.7507, 0.7480, 0.7409, 0.7395, 0.7382, 0.7371,\n",
      "         0.7367]], device='mps:0')\n",
      "######## in VL03482_019.jp2   || Top K similarity values: tensor([[0.7566, 0.7473, 0.7469, 0.7396, 0.7347, 0.7344, 0.7270, 0.7258, 0.7257,\n",
      "         0.7225]], device='mps:0')\n",
      "MATCH: 33 in VL49150_055.jp2   || Top K similarity values: tensor([[0.7519, 0.7516, 0.7512, 0.7505, 0.7489, 0.7466, 0.7453, 0.7421, 0.7418,\n",
      "         0.7411]], device='mps:0')\n",
      "MATCH: 34 in VL05124_148.jp2   || Top K similarity values: tensor([[0.7324, 0.7292, 0.7281, 0.7276, 0.7272, 0.7190, 0.7178, 0.7177, 0.7170,\n",
      "         0.7151]], device='mps:0')\n",
      "MATCH: 35 in VL45750_175.jp2   || Top K similarity values: tensor([[0.7747, 0.7520, 0.7519, 0.7508, 0.7495, 0.7472, 0.7466, 0.7456, 0.7432,\n",
      "         0.7396]], device='mps:0')\n",
      "MATCH: 36 in VL02210_034.jp2   || Top K similarity values: tensor([[0.7749, 0.7721, 0.7702, 0.7628, 0.7616, 0.7572, 0.7571, 0.7550, 0.7503,\n",
      "         0.7485]], device='mps:0')\n",
      "######## in VL00625_272.jp2   || Top K similarity values: tensor([[0.7830, 0.7719, 0.7714, 0.7681, 0.7664, 0.7663, 0.7656, 0.7630, 0.7595,\n",
      "         0.7577]], device='mps:0')\n",
      "MATCH: 37 in VL03541_225.jp2   || Top K similarity values: tensor([[0.7691, 0.7684, 0.7637, 0.7618, 0.7562, 0.7486, 0.7478, 0.7460, 0.7454,\n",
      "         0.7435]], device='mps:0')\n",
      "MATCH: 38 in VL01178_253.jp2   || Top K similarity values: tensor([[0.8026, 0.7829, 0.7721, 0.7444, 0.7441, 0.7412, 0.7399, 0.7387, 0.7375,\n",
      "         0.7365]], device='mps:0')\n",
      "MATCH: 39 in VL01178_247.jp2   || Top K similarity values: tensor([[0.8001, 0.7832, 0.7655, 0.7516, 0.7509, 0.7507, 0.7498, 0.7473, 0.7407,\n",
      "         0.7399]], device='mps:0')\n",
      "MATCH: 40 in VL08682_009.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "MATCH: 41 in VL0H907_145.jp2   || Top K similarity values: tensor([[0.7348, 0.7247, 0.7217, 0.7182, 0.7161, 0.7160, 0.7157, 0.7139, 0.7129,\n",
      "         0.7118]], device='mps:0')\n",
      "MATCH: 42 in VL04490_055.jp2   || Top K similarity values: tensor([[0.7037, 0.6962, 0.6955, 0.6906, 0.6896, 0.6876, 0.6873, 0.6850, 0.6847,\n",
      "         0.6843]], device='mps:0')\n",
      "######## in VL40450_179.jp2   || Top K similarity values: tensor([[0.7780, 0.7663, 0.7625, 0.7608, 0.7605, 0.7597, 0.7579, 0.7538, 0.7527,\n",
      "         0.7485]], device='mps:0')\n",
      "MATCH: 43 in VL0H628_139.jp2   || Top K similarity values: tensor([[0.7304, 0.7208, 0.6998, 0.6954, 0.6914, 0.6908, 0.6871, 0.6840, 0.6835,\n",
      "         0.6779]], device='mps:0')\n",
      "MATCH: 44 in VL45750_216.jp2   || Top K similarity values: tensor([[0.7915, 0.7915, 0.7880, 0.7876, 0.7871, 0.7846, 0.7816, 0.7770, 0.7757,\n",
      "         0.7732]], device='mps:0')\n",
      "MATCH: 45 in VL00927_097.jp2   || Top K similarity values: tensor([[0.7693, 0.7604, 0.7515, 0.7496, 0.7466, 0.7443, 0.7396, 0.7325, 0.7313,\n",
      "         0.7281]], device='mps:0')\n",
      "MATCH: 46 in VL45750_174.jp2   || Top K similarity values: tensor([[0.7763, 0.7567, 0.7552, 0.7510, 0.7497, 0.7495, 0.7450, 0.7450, 0.7428,\n",
      "         0.7415]], device='mps:0')\n",
      "MATCH: 47 in VL40850_333.jp2   || Top K similarity values: tensor([[0.7569, 0.7504, 0.7446, 0.7444, 0.7418, 0.7390, 0.7383, 0.7376, 0.7326,\n",
      "         0.7322]], device='mps:0')\n",
      "MATCH: 48 in VL00921_207.jp2   || Top K similarity values: tensor([[0.7613, 0.7522, 0.7521, 0.7476, 0.7419, 0.7392, 0.7365, 0.7359, 0.7354,\n",
      "         0.7350]], device='mps:0')\n",
      "MATCH: 49 in VL02925_051.jp2   || Top K similarity values: tensor([[0.7714, 0.7608, 0.7542, 0.7495, 0.7489, 0.7474, 0.7446, 0.7405, 0.7403,\n",
      "         0.7338]], device='mps:0')\n",
      "MATCH: 50 in VLA0196_116.jp2   || Top K similarity values: tensor([[0.7539, 0.7512, 0.7452, 0.7417, 0.7380, 0.7343, 0.7312, 0.7301, 0.7301,\n",
      "         0.7276]], device='mps:0')\n",
      "MATCH: 51 in VL79500_094.jp2   || Top K similarity values: tensor([[0.7591, 0.7514, 0.7514, 0.7507, 0.7482, 0.7477, 0.7449, 0.7435, 0.7424,\n",
      "         0.7420]], device='mps:0')\n",
      "MATCH: 52 in VL08258_063.jp2   || Top K similarity values: tensor([[0.7308, 0.7291, 0.7063, 0.7046, 0.7004, 0.6954, 0.6951, 0.6945, 0.6937,\n",
      "         0.6937]], device='mps:0')\n",
      "MATCH: 53 in VL03541_226.jp2   || Top K similarity values: tensor([[0.7862, 0.7835, 0.7776, 0.7718, 0.7714, 0.7689, 0.7651, 0.7605, 0.7585,\n",
      "         0.7561]], device='mps:0')\n",
      "MATCH: 54 in VL04735_092.jp2   || Top K similarity values: tensor([[0.7309, 0.7291, 0.7246, 0.7192, 0.7031, 0.7007, 0.6994, 0.6986, 0.6977,\n",
      "         0.6964]], device='mps:0')\n",
      "######## in VL00024_071.jp2   || Top K similarity values: tensor([[0.8392, 0.8224, 0.8071, 0.8038, 0.8027, 0.7959, 0.7955, 0.7924, 0.7917,\n",
      "         0.7895]], device='mps:0')\n",
      "######## in VL02425_126.jp2   || Top K similarity values: tensor([[0.7126, 0.7067, 0.6993, 0.6953, 0.6942, 0.6935, 0.6927, 0.6897, 0.6897,\n",
      "         0.6890]], device='mps:0')\n",
      "######## in VL53700_045.jp2   || Top K similarity values: tensor([[0.7639, 0.7575, 0.7518, 0.7508, 0.7491, 0.7397, 0.7378, 0.7360, 0.7333,\n",
      "         0.7321]], device='mps:0')\n",
      "MATCH: 55 in VL00006_325.jp2   || Top K similarity values: tensor([[0.7769, 0.7730, 0.7634, 0.7631, 0.7565, 0.7549, 0.7510, 0.7490, 0.7456,\n",
      "         0.7444]], device='mps:0')\n",
      "######## in VL02863_055.jp2   || Top K similarity values: tensor([[0.7587, 0.7523, 0.7481, 0.7448, 0.7442, 0.7433, 0.7431, 0.7410, 0.7404,\n",
      "         0.7380]], device='mps:0')\n",
      "######## in VL49200_135.jp2   || Top K similarity values: tensor([[0.7363, 0.7327, 0.7251, 0.7250, 0.7224, 0.7220, 0.7215, 0.7208, 0.7187,\n",
      "         0.7164]], device='mps:0')\n",
      "MATCH: 56 in VLH1128_138.jp2   || Top K similarity values: tensor([[0.8049, 0.8048, 0.8046, 0.7942, 0.7920, 0.7901, 0.7893, 0.7882, 0.7870,\n",
      "         0.7834]], device='mps:0')\n",
      "MATCH: 57 in VL08008_215.jp2   || Top K similarity values: tensor([[0.7754, 0.7670, 0.7630, 0.7483, 0.7462, 0.7456, 0.7410, 0.7391, 0.7388,\n",
      "         0.7373]], device='mps:0')\n",
      "MATCH: 58 in VL02074_144.jp2   || Top K similarity values: tensor([[0.8140, 0.7740, 0.7597, 0.7522, 0.7501, 0.7489, 0.7471, 0.7417, 0.7405,\n",
      "         0.7404]], device='mps:0')\n",
      "MATCH: 59 in VL02579_154.jp2   || Top K similarity values: tensor([[0.7838, 0.7731, 0.7652, 0.7628, 0.7609, 0.7604, 0.7601, 0.7569, 0.7553,\n",
      "         0.7541]], device='mps:0')\n",
      "MATCH: 60 in VL02481_070.jp2   || Top K similarity values: tensor([[0.8044, 0.7617, 0.7480, 0.7407, 0.7392, 0.7366, 0.7349, 0.7320, 0.7309,\n",
      "         0.7296]], device='mps:0')\n",
      "######## in VL00815_200.jp2   || Top K similarity values: tensor([[0.7595, 0.7420, 0.7418, 0.7404, 0.7385, 0.7358, 0.7346, 0.7337, 0.7337,\n",
      "         0.7318]], device='mps:0')\n",
      "MATCH: 61 in VL03826_163.jp2   || Top K similarity values: tensor([[0.7788, 0.7712, 0.7629, 0.7578, 0.7532, 0.7458, 0.7453, 0.7433, 0.7423,\n",
      "         0.7410]], device='mps:0')\n",
      "MATCH: 62 in VLH1421_087.jp2   || Top K similarity values: tensor([[0.7559, 0.7469, 0.7445, 0.7403, 0.7271, 0.7239, 0.7227, 0.7196, 0.7175,\n",
      "         0.7164]], device='mps:0')\n",
      "######## in VL0H703_140.jp2   || Top K similarity values: tensor([[0.7547, 0.7546, 0.7522, 0.7519, 0.7456, 0.7454, 0.7449, 0.7443, 0.7417,\n",
      "         0.7410]], device='mps:0')\n",
      "MATCH: 63 in VL0H524_272.jp2   || Top K similarity values: tensor([[0.7702, 0.7486, 0.7456, 0.7424, 0.7354, 0.7341, 0.7243, 0.7236, 0.7226,\n",
      "         0.7206]], device='mps:0')\n",
      "MATCH: 64 in VL00927_084.jp2   || Top K similarity values: tensor([[0.7952, 0.7491, 0.7430, 0.7429, 0.7418, 0.7375, 0.7373, 0.7367, 0.7355,\n",
      "         0.7252]], device='mps:0')\n",
      "MATCH: 65 in VL00690_246.jp2   || Top K similarity values: tensor([[0.7535, 0.7452, 0.7414, 0.7406, 0.7364, 0.7356, 0.7346, 0.7311, 0.7277,\n",
      "         0.7262]], device='mps:0')\n",
      "MATCH: 66 in VL00760_204.jp2   || Top K similarity values: tensor([[0.7319, 0.7285, 0.7115, 0.7093, 0.7058, 0.7058, 0.6997, 0.6994, 0.6994,\n",
      "         0.6992]], device='mps:0')\n",
      "MATCH: 67 in VL00792_254.jp2   || Top K similarity values: tensor([[0.7759, 0.7588, 0.7577, 0.7527, 0.7487, 0.7450, 0.7442, 0.7439, 0.7427,\n",
      "         0.7409]], device='mps:0')\n",
      "MATCH: 68 in VL50450_064.jp2   || Top K similarity values: tensor([[0.7463, 0.7391, 0.7372, 0.7356, 0.7299, 0.7284, 0.7277, 0.7275, 0.7269,\n",
      "         0.7266]], device='mps:0')\n",
      "MATCH: 69 in VL04636_095.jp2   || Top K similarity values: tensor([[0.7272, 0.7236, 0.7187, 0.7122, 0.7108, 0.7086, 0.7080, 0.7053, 0.7049,\n",
      "         0.7049]], device='mps:0')\n",
      "######## in VL00578_136.jp2   || Top K similarity values: tensor([[0.6608, 0.6533, 0.6419, 0.6358, 0.6355, 0.6330, 0.6303, 0.6301, 0.6276,\n",
      "         0.6274]], device='mps:0')\n",
      "MATCH: 70 in VL03988_192.jp2   || Top K similarity values: tensor([[0.7917, 0.7884, 0.7879, 0.7844, 0.7827, 0.7741, 0.7710, 0.7640, 0.7636,\n",
      "         0.7625]], device='mps:0')\n",
      "MATCH: 71 in VL00535_137.jp2   || Top K similarity values: tensor([[0.7287, 0.7254, 0.7228, 0.7197, 0.7106, 0.7088, 0.7056, 0.7010, 0.6999,\n",
      "         0.6962]], device='mps:0')\n",
      "MATCH: 72 in VL08094_138.jp2   || Top K similarity values: tensor([[0.7472, 0.7310, 0.7310, 0.7307, 0.7301, 0.7252, 0.7124, 0.7088, 0.7060,\n",
      "         0.7032]], device='mps:0')\n",
      "######## in VL42250_085.jp2   || Top K similarity values: tensor([[0.7319, 0.7290, 0.7261, 0.7226, 0.7212, 0.7187, 0.7163, 0.7123, 0.7120,\n",
      "         0.7115]], device='mps:0')\n",
      "######## in VL02579_151.jp2   || Top K similarity values: tensor([[0.7554, 0.7497, 0.7425, 0.7418, 0.7403, 0.7398, 0.7348, 0.7338, 0.7337,\n",
      "         0.7310]], device='mps:0')\n",
      "MATCH: 73 in VL03636_179.jp2   || Top K similarity values: tensor([[0.7908, 0.7706, 0.7662, 0.7566, 0.7546, 0.7529, 0.7529, 0.7528, 0.7527,\n",
      "         0.7522]], device='mps:0')\n",
      "MATCH: 74 in VL00760_205.jp2   || Top K similarity values: tensor([[0.7887, 0.7885, 0.7872, 0.7825, 0.7821, 0.7794, 0.7787, 0.7783, 0.7758,\n",
      "         0.7724]], device='mps:0')\n",
      "######## in VLH1214_034.jp2   || Top K similarity values: tensor([[0.7552, 0.7534, 0.7385, 0.7309, 0.7303, 0.7282, 0.7264, 0.7252, 0.7235,\n",
      "         0.7221]], device='mps:0')\n",
      "MATCH: 75 in VL03816_135.jp2   || Top K similarity values: tensor([[0.7606, 0.7552, 0.7436, 0.7374, 0.7338, 0.7336, 0.7312, 0.7268, 0.7260,\n",
      "         0.7225]], device='mps:0')\n",
      "MATCH: 76 in VL01506_020.jp2   || Top K similarity values: tensor([[0.7591, 0.7399, 0.7320, 0.7272, 0.7236, 0.7223, 0.7206, 0.7197, 0.7194,\n",
      "         0.7187]], device='mps:0')\n",
      "MATCH: 77 in VL00921_203.jp2   || Top K similarity values: tensor([[0.7529, 0.7306, 0.7289, 0.7243, 0.7229, 0.7226, 0.7225, 0.7200, 0.7191,\n",
      "         0.7178]], device='mps:0')\n",
      "MATCH: 78 in VLH1421_053.jp2   || Top K similarity values: tensor([[0.7590, 0.7467, 0.7345, 0.7341, 0.7319, 0.7307, 0.7263, 0.7262, 0.7257,\n",
      "         0.7257]], device='mps:0')\n",
      "######## in VL03852_041.jp2   || Top K similarity values: tensor([[0.7321, 0.7239, 0.7230, 0.7145, 0.7131, 0.7012, 0.6998, 0.6985, 0.6982,\n",
      "         0.6959]], device='mps:0')\n",
      "MATCH: 79 in VL00014_385.jp2   || Top K similarity values: tensor([[0.7865, 0.7807, 0.7725, 0.7635, 0.7554, 0.7549, 0.7547, 0.7509, 0.7485,\n",
      "         0.7484]], device='mps:0')\n",
      "######## in VL00503_123.jp2   || Top K similarity values: tensor([[0.7618, 0.7617, 0.7614, 0.7521, 0.7424, 0.7421, 0.7377, 0.7375, 0.7341,\n",
      "         0.7338]], device='mps:0')\n",
      "MATCH: 80 in VL03686_187.jp2   || Top K similarity values: tensor([[0.7838, 0.7693, 0.7550, 0.7533, 0.7490, 0.7477, 0.7476, 0.7475, 0.7472,\n",
      "         0.7449]], device='mps:0')\n",
      "######## in VL01624_034.jp2   || Top K similarity values: tensor([[0.7434, 0.7327, 0.7317, 0.7303, 0.7244, 0.7229, 0.7222, 0.7203, 0.7194,\n",
      "         0.7166]], device='mps:0')\n",
      "MATCH: 81 in VL78350_187.jp2   || Top K similarity values: tensor([[0.7795, 0.7635, 0.7547, 0.7512, 0.7476, 0.7472, 0.7463, 0.7460, 0.7458,\n",
      "         0.7437]], device='mps:0')\n",
      "MATCH: 82 in VL69600_220.jp2   || Top K similarity values: tensor([[0.7535, 0.7508, 0.7497, 0.7488, 0.7460, 0.7458, 0.7442, 0.7393, 0.7340,\n",
      "         0.7335]], device='mps:0')\n",
      "######## in VL04295_099.jp2   || Top K similarity values: tensor([[0.7666, 0.7665, 0.7635, 0.7615, 0.7589, 0.7559, 0.7554, 0.7535, 0.7477,\n",
      "         0.7462]], device='mps:0')\n",
      "MATCH: 83 in VL00012_276.jp2   || Top K similarity values: tensor([[0.7446, 0.7435, 0.7356, 0.7341, 0.7333, 0.7314, 0.7307, 0.7275, 0.7260,\n",
      "         0.7222]], device='mps:0')\n",
      "MATCH: 84 in VL03641_177.jp2   || Top K similarity values: tensor([[0.7670, 0.7353, 0.7204, 0.7190, 0.7133, 0.7115, 0.7079, 0.7078, 0.7077,\n",
      "         0.7055]], device='mps:0')\n",
      "MATCH: 85 in VL03263_040.jp2   || Top K similarity values: tensor([[0.7933, 0.7930, 0.7929, 0.7920, 0.7879, 0.7873, 0.7842, 0.7830, 0.7821,\n",
      "         0.7811]], device='mps:0')\n",
      "MATCH: 86 in VL01592_062.jp2   || Top K similarity values: tensor([[0.7711, 0.7515, 0.7515, 0.7510, 0.7420, 0.7418, 0.7408, 0.7401, 0.7400,\n",
      "         0.7390]], device='mps:0')\n",
      "MATCH: 87 in VL04735_067.jp2   || Top K similarity values: tensor([[0.7113, 0.7097, 0.7095, 0.7008, 0.7001, 0.6980, 0.6975, 0.6948, 0.6934,\n",
      "         0.6901]], device='mps:0')\n",
      "MATCH: 88 in VL01682_122.jp2   || Top K similarity values: tensor([[0.8139, 0.7809, 0.7773, 0.7637, 0.7617, 0.7564, 0.7534, 0.7528, 0.7520,\n",
      "         0.7520]], device='mps:0')\n",
      "######## in VL41050_004.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL04490_048.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL00564_453.jp2   || Top K similarity values: tensor([[0.7406, 0.7306, 0.7261, 0.7252, 0.7228, 0.7225, 0.7216, 0.7209, 0.7198,\n",
      "         0.7172]], device='mps:0')\n",
      "MATCH: 89 in VL53500_145.jp2   || Top K similarity values: tensor([[0.7206, 0.7173, 0.7131, 0.7112, 0.7071, 0.7058, 0.7024, 0.7019, 0.6985,\n",
      "         0.6964]], device='mps:0')\n",
      "MATCH: 90 in VL73650_210.jp2   || Top K similarity values: tensor([[0.7802, 0.7424, 0.7395, 0.7382, 0.7360, 0.7283, 0.7260, 0.7260, 0.7246,\n",
      "         0.7246]], device='mps:0')\n",
      "MATCH: 91 in VL03263_041.jp2   || Top K similarity values: tensor([[0.7628, 0.7563, 0.7506, 0.7469, 0.7464, 0.7421, 0.7419, 0.7417, 0.7402,\n",
      "         0.7397]], device='mps:0')\n",
      "######## in VL01582_088.jp2   || Top K similarity values: tensor([[0.7508, 0.7474, 0.7467, 0.7420, 0.7370, 0.7363, 0.7353, 0.7342, 0.7329,\n",
      "         0.7318]], device='mps:0')\n",
      "MATCH: 92 in VL0H313_136.jp2   || Top K similarity values: tensor([[0.6574, 0.6485, 0.6450, 0.6429, 0.6424, 0.6300, 0.6297, 0.6294, 0.6294,\n",
      "         0.6260]], device='mps:0')\n",
      "MATCH: 93 in VL40550_113.jp2   || Top K similarity values: tensor([[0.7881, 0.7562, 0.7535, 0.7430, 0.7413, 0.7370, 0.7366, 0.7365, 0.7346,\n",
      "         0.7339]], device='mps:0')\n",
      "MATCH: 94 in VL00963_129.jp2   || Top K similarity values: tensor([[0.7893, 0.7813, 0.7747, 0.7746, 0.7723, 0.7719, 0.7709, 0.7687, 0.7658,\n",
      "         0.7646]], device='mps:0')\n",
      "MATCH: 95 in VL03639_244.jp2   || Top K similarity values: tensor([[0.7455, 0.7349, 0.7264, 0.7243, 0.7227, 0.7200, 0.7170, 0.7150, 0.7138,\n",
      "         0.7136]], device='mps:0')\n",
      "MATCH: 96 in VL01178_305.jp2   || Top K similarity values: tensor([[0.8073, 0.7936, 0.7808, 0.7498, 0.7442, 0.7437, 0.7432, 0.7403, 0.7399,\n",
      "         0.7383]], device='mps:0')\n",
      "MATCH: 97 in VL03686_188.jp2   || Top K similarity values: tensor([[0.7454, 0.7451, 0.7450, 0.7364, 0.7331, 0.7327, 0.7324, 0.7315, 0.7306,\n",
      "         0.7301]], device='mps:0')\n",
      "MATCH: 98 in VL53500_147.jp2   || Top K similarity values: tensor([[0.7507, 0.7465, 0.7436, 0.7425, 0.7410, 0.7346, 0.7340, 0.7273, 0.7235,\n",
      "         0.7226]], device='mps:0')\n",
      "MATCH: 99 in VL00755_054.jp2   || Top K similarity values: tensor([[0.7469, 0.7313, 0.7239, 0.7232, 0.7231, 0.7229, 0.7224, 0.7217, 0.7213,\n",
      "         0.7195]], device='mps:0')\n",
      "MATCH: 100 in VL00578_112.jp2   || Top K similarity values: tensor([[0.6723, 0.6618, 0.6554, 0.6471, 0.6463, 0.6422, 0.6420, 0.6415, 0.6414,\n",
      "         0.6413]], device='mps:0')\n",
      "MATCH: 101 in VL02100_157.jp2   || Top K similarity values: tensor([[0.7694, 0.7681, 0.7673, 0.7648, 0.7628, 0.7594, 0.7584, 0.7521, 0.7519,\n",
      "         0.7487]], device='mps:0')\n",
      "MATCH: 102 in VL00017_300.jp2   || Top K similarity values: tensor([[0.7265, 0.7204, 0.7155, 0.7152, 0.7117, 0.7100, 0.7014, 0.7004, 0.6996,\n",
      "         0.6993]], device='mps:0')\n",
      "MATCH: 103 in VL03635_101.jp2   || Top K similarity values: tensor([[0.7897, 0.7884, 0.7618, 0.7505, 0.7485, 0.7465, 0.7423, 0.7395, 0.7377,\n",
      "         0.7350]], device='mps:0')\n",
      "MATCH: 104 in VL00963_100.jp2   || Top K similarity values: tensor([[0.7693, 0.7574, 0.7423, 0.7400, 0.7380, 0.7325, 0.7323, 0.7308, 0.7306,\n",
      "         0.7302]], device='mps:0')\n",
      "######## in VL0H907_210.jp2   || Top K similarity values: tensor([[0.7355, 0.7335, 0.7292, 0.7273, 0.7271, 0.7271, 0.7257, 0.7253, 0.7241,\n",
      "         0.7223]], device='mps:0')\n",
      "MATCH: 105 in VL04592_129.jp2   || Top K similarity values: tensor([[0.7691, 0.7536, 0.7381, 0.7361, 0.7344, 0.7279, 0.7252, 0.7244, 0.7238,\n",
      "         0.7224]], device='mps:0')\n",
      "MATCH: 106 in VL02299_068.jp2   || Top K similarity values: tensor([[0.7612, 0.7571, 0.7514, 0.7493, 0.7464, 0.7459, 0.7457, 0.7433, 0.7392,\n",
      "         0.7350]], device='mps:0')\n",
      "MATCH: 107 in VL00758_106.jp2   || Top K similarity values: tensor([[0.7971, 0.7636, 0.7631, 0.7590, 0.7560, 0.7538, 0.7531, 0.7466, 0.7463,\n",
      "         0.7417]], device='mps:0')\n",
      "######## in VL00575_096.jp2   || Top K similarity values: tensor([[0.7279, 0.7181, 0.7165, 0.7141, 0.7140, 0.7135, 0.7135, 0.7128, 0.7116,\n",
      "         0.7111]], device='mps:0')\n",
      "######## in VL01290_111.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL0H703_175.jp2   || Top K similarity values: tensor([[0.7494, 0.7489, 0.7460, 0.7443, 0.7437, 0.7418, 0.7410, 0.7399, 0.7398,\n",
      "         0.7391]], device='mps:0')\n",
      "MATCH: 108 in VL01474_088.jp2   || Top K similarity values: tensor([[0.7588, 0.7542, 0.7512, 0.7466, 0.7463, 0.7453, 0.7452, 0.7443, 0.7438,\n",
      "         0.7435]], device='mps:0')\n",
      "MATCH: 109 in VL00633_402.jp2   || Top K similarity values: tensor([[0.7828, 0.7751, 0.7723, 0.7699, 0.7698, 0.7695, 0.7660, 0.7648, 0.7627,\n",
      "         0.7619]], device='mps:0')\n",
      "MATCH: 110 in VL0H628_137.jp2   || Top K similarity values: tensor([[0.7068, 0.6964, 0.6960, 0.6949, 0.6918, 0.6871, 0.6817, 0.6787, 0.6766,\n",
      "         0.6753]], device='mps:0')\n",
      "######## in VL00562_149.jp2   || Top K similarity values: tensor([[0.7451, 0.7387, 0.7251, 0.7242, 0.7217, 0.7193, 0.7188, 0.7187, 0.7097,\n",
      "         0.7097]], device='mps:0')\n",
      "MATCH: 111 in VL44050_371.jp2   || Top K similarity values: tensor([[0.7659, 0.7451, 0.7393, 0.7380, 0.7374, 0.7361, 0.7360, 0.7356, 0.7337,\n",
      "         0.7313]], device='mps:0')\n",
      "######## in VL00633_403.jp2   || Top K similarity values: tensor([[0.7889, 0.7757, 0.7719, 0.7629, 0.7620, 0.7603, 0.7602, 0.7554, 0.7528,\n",
      "         0.7512]], device='mps:0')\n",
      "MATCH: 112 in VL04735_060.jp2   || Top K similarity values: tensor([[0.7042, 0.6966, 0.6697, 0.6667, 0.6524, 0.6506, 0.6504, 0.6468, 0.6423,\n",
      "         0.6404]], device='mps:0')\n",
      "MATCH: 113 in VLA0517_042.jp2   || Top K similarity values: tensor([[0.7167, 0.7131, 0.7074, 0.7063, 0.6973, 0.6950, 0.6949, 0.6930, 0.6928,\n",
      "         0.6924]], device='mps:0')\n",
      "######## in VL0H703_174.jp2   || Top K similarity values: tensor([[0.7378, 0.7275, 0.7260, 0.7245, 0.7244, 0.7197, 0.7192, 0.7185, 0.7185,\n",
      "         0.7183]], device='mps:0')\n",
      "MATCH: 114 in VL50100_069.jp2   || Top K similarity values: tensor([[0.7707, 0.7695, 0.7643, 0.7584, 0.7581, 0.7478, 0.7468, 0.7458, 0.7438,\n",
      "         0.7427]], device='mps:0')\n",
      "######## in VL0H313_132.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL01474_048.jp2   || Top K similarity values: tensor([[0.7505, 0.7499, 0.7390, 0.7317, 0.7288, 0.7273, 0.7263, 0.7263, 0.7255,\n",
      "         0.7224]], device='mps:0')\n",
      "######## in VL00040_167.jp2   || Top K similarity values: tensor([[0.7465, 0.7423, 0.7411, 0.7361, 0.7359, 0.7352, 0.7350, 0.7329, 0.7327,\n",
      "         0.7327]], device='mps:0')\n",
      "MATCH: 115 in VL78350_165.jp2   || Top K similarity values: tensor([[0.7579, 0.7545, 0.7520, 0.7474, 0.7468, 0.7467, 0.7430, 0.7426, 0.7400,\n",
      "         0.7385]], device='mps:0')\n",
      "######## in VL0H905_266.jp2   || Top K similarity values: tensor([[0.7809, 0.7755, 0.7714, 0.7700, 0.7614, 0.7613, 0.7612, 0.7577, 0.7550,\n",
      "         0.7531]], device='mps:0')\n",
      "MATCH: 116 in VL00926_129.jp2   || Top K similarity values: tensor([[0.7553, 0.7538, 0.7506, 0.7486, 0.7469, 0.7391, 0.7346, 0.7327, 0.7320,\n",
      "         0.7319]], device='mps:0')\n",
      "MATCH: 117 in VL0H450_129.jp2   || Top K similarity values: tensor([[0.7573, 0.7441, 0.7245, 0.7215, 0.7189, 0.7143, 0.7117, 0.7112, 0.7108,\n",
      "         0.7107]], device='mps:0')\n",
      "######## in VL08759_073.jp2   || Top K similarity values: tensor([[0.7419, 0.7413, 0.7384, 0.7270, 0.7239, 0.7229, 0.7218, 0.7195, 0.7165,\n",
      "         0.7160]], device='mps:0')\n",
      "######## in VL00815_192.jp2   || Top K similarity values: tensor([[0.7613, 0.7506, 0.7490, 0.7482, 0.7469, 0.7451, 0.7435, 0.7424, 0.7416,\n",
      "         0.7409]], device='mps:0')\n",
      "MATCH: 118 in VL03698_133.jp2   || Top K similarity values: tensor([[0.7556, 0.7461, 0.7426, 0.7394, 0.7375, 0.7365, 0.7317, 0.7311, 0.7308,\n",
      "         0.7274]], device='mps:0')\n",
      "######## in VL49250_050.jp2   || Top K similarity values: tensor([[0.7694, 0.7607, 0.7412, 0.7411, 0.7379, 0.7278, 0.7261, 0.7247, 0.7232,\n",
      "         0.7227]], device='mps:0')\n",
      "######## in VL02451_151.jp2   || Top K similarity values: tensor([[0.7650, 0.7527, 0.7466, 0.7450, 0.7450, 0.7417, 0.7416, 0.7394, 0.7371,\n",
      "         0.7352]], device='mps:0')\n",
      "MATCH: 119 in VL01206_029.jp2   || Top K similarity values: tensor([[0.7479, 0.7394, 0.7390, 0.7366, 0.7354, 0.7336, 0.7318, 0.7309, 0.7302,\n",
      "         0.7278]], device='mps:0')\n",
      "MATCH: 120 in VLH1363_180.jp2   || Top K similarity values: tensor([[0.7649, 0.7302, 0.7258, 0.7217, 0.7191, 0.7170, 0.7127, 0.7118, 0.7106,\n",
      "         0.7088]], device='mps:0')\n",
      "######## in VL58650_065.jp2   || Top K similarity values: tensor([[0.7646, 0.7467, 0.7462, 0.7382, 0.7381, 0.7357, 0.7338, 0.7313, 0.7288,\n",
      "         0.7271]], device='mps:0')\n",
      "######## in VL0H516_162.jp2   || Top K similarity values: tensor([[0.7518, 0.7454, 0.7368, 0.7366, 0.7358, 0.7343, 0.7323, 0.7308, 0.7291,\n",
      "         0.7281]], device='mps:0')\n",
      "######## in VL00001_278.jp2   || Top K similarity values: tensor([[0.7309, 0.7247, 0.7231, 0.7226, 0.7224, 0.7193, 0.7192, 0.7168, 0.7162,\n",
      "         0.7154]], device='mps:0')\n",
      "MATCH: 121 in VL01070_116.jp2   || Top K similarity values: tensor([[0.7695, 0.7634, 0.7582, 0.7560, 0.7513, 0.7480, 0.7476, 0.7468, 0.7444,\n",
      "         0.7439]], device='mps:0')\n",
      "MATCH: 122 in VL0H600_073.jp2   || Top K similarity values: tensor([[0.7265, 0.7173, 0.7128, 0.7017, 0.6959, 0.6945, 0.6918, 0.6904, 0.6898,\n",
      "         0.6889]], device='mps:0')\n",
      "MATCH: 123 in VL00768_093.jp2   || Top K similarity values: tensor([[0.7729, 0.7627, 0.7574, 0.7553, 0.7548, 0.7501, 0.7489, 0.7479, 0.7445,\n",
      "         0.7445]], device='mps:0')\n",
      "MATCH: 124 in VL02946_160.jp2   || Top K similarity values: tensor([[0.7756, 0.7700, 0.7633, 0.7621, 0.7600, 0.7588, 0.7583, 0.7552, 0.7527,\n",
      "         0.7523]], device='mps:0')\n",
      "######## in VL00959_187.jp2   || Top K similarity values: tensor([[0.7341, 0.7225, 0.7215, 0.7202, 0.7170, 0.7144, 0.7119, 0.7114, 0.7102,\n",
      "         0.7102]], device='mps:0')\n",
      "MATCH: 125 in VL44450_032.jp2   || Top K similarity values: tensor([[0.7742, 0.7684, 0.7684, 0.7629, 0.7554, 0.7539, 0.7528, 0.7523, 0.7514,\n",
      "         0.7513]], device='mps:0')\n",
      "MATCH: 126 in VL41550_040.jp2   || Top K similarity values: tensor([[0.7723, 0.7688, 0.7652, 0.7652, 0.7573, 0.7571, 0.7559, 0.7549, 0.7546,\n",
      "         0.7544]], device='mps:0')\n",
      "MATCH: 127 in VL00954_173.jp2   || Top K similarity values: tensor([[0.7295, 0.7203, 0.7196, 0.7190, 0.7190, 0.7177, 0.7166, 0.7165, 0.7133,\n",
      "         0.7131]], device='mps:0')\n",
      "######## in VL00736_230.jp2   || Top K similarity values: tensor([[0.7487, 0.7432, 0.7348, 0.7346, 0.7291, 0.7288, 0.7258, 0.7258, 0.7242,\n",
      "         0.7235]], device='mps:0')\n",
      "######## in VL03610_061.jp2   || Top K similarity values: tensor([[0.7331, 0.7261, 0.7246, 0.7160, 0.7158, 0.7087, 0.7086, 0.7031, 0.7031,\n",
      "         0.7030]], device='mps:0')\n",
      "MATCH: 128 in VL03586_120.jp2   || Top K similarity values: tensor([[0.7619, 0.7466, 0.7422, 0.7389, 0.7389, 0.7310, 0.7279, 0.7278, 0.7251,\n",
      "         0.7250]], device='mps:0')\n",
      "MATCH: 129 in VL00770_135.jp2   || Top K similarity values: tensor([[0.7451, 0.7439, 0.7381, 0.7377, 0.7363, 0.7354, 0.7328, 0.7319, 0.7309,\n",
      "         0.7302]], device='mps:0')\n",
      "MATCH: 130 in VL01092_233.jp2   || Top K similarity values: tensor([[0.7348, 0.7345, 0.7323, 0.7304, 0.7170, 0.7156, 0.7134, 0.7095, 0.7085,\n",
      "         0.7083]], device='mps:0')\n",
      "MATCH: 131 in VL03595_035.jp2   || Top K similarity values: tensor([[0.7593, 0.7481, 0.7415, 0.7395, 0.7369, 0.7339, 0.7332, 0.7281, 0.7260,\n",
      "         0.7244]], device='mps:0')\n",
      "######## in VL08676_060.jp2   || Top K similarity values: tensor([[0.7530, 0.7463, 0.7355, 0.7331, 0.7317, 0.7311, 0.7262, 0.7255, 0.7254,\n",
      "         0.7250]], device='mps:0')\n",
      "MATCH: 132 in VL00990_081.jp2   || Top K similarity values: tensor([[0.7588, 0.7578, 0.7536, 0.7454, 0.7379, 0.7368, 0.7349, 0.7340, 0.7334,\n",
      "         0.7330]], device='mps:0')\n",
      "MATCH: 133 in VL04531_059.jp2   || Top K similarity values: tensor([[0.7705, 0.7703, 0.7696, 0.7681, 0.7623, 0.7620, 0.7591, 0.7546, 0.7545,\n",
      "         0.7535]], device='mps:0')\n",
      "MATCH: 134 in VL0H599_214.jp2   || Top K similarity values: tensor([[0.7430, 0.7301, 0.7243, 0.7224, 0.7177, 0.7177, 0.7175, 0.7167, 0.7164,\n",
      "         0.7095]], device='mps:0')\n",
      "MATCH: 135 in VL69400_179.jp2   || Top K similarity values: tensor([[0.7887, 0.7823, 0.7771, 0.7630, 0.7598, 0.7576, 0.7569, 0.7547, 0.7547,\n",
      "         0.7538]], device='mps:0')\n",
      "######## in VL69400_145.jp2   || Top K similarity values: tensor([[0.7838, 0.7813, 0.7737, 0.7585, 0.7575, 0.7565, 0.7557, 0.7529, 0.7512,\n",
      "         0.7499]], device='mps:0')\n",
      "MATCH: 136 in VL00792_206.jp2   || Top K similarity values: tensor([[0.8020, 0.7993, 0.7918, 0.7855, 0.7840, 0.7822, 0.7788, 0.7778, 0.7776,\n",
      "         0.7764]], device='mps:0')\n",
      "MATCH: 137 in VL01149_133.jp2   || Top K similarity values: tensor([[0.7359, 0.6824, 0.6791, 0.6761, 0.6596, 0.6588, 0.6496, 0.6487, 0.6449,\n",
      "         0.6444]], device='mps:0')\n",
      "MATCH: 138 in VL00549_269.jp2   || Top K similarity values: tensor([[0.7932, 0.7816, 0.7777, 0.7701, 0.7700, 0.7662, 0.7656, 0.7644, 0.7634,\n",
      "         0.7624]], device='mps:0')\n",
      "######## in VL55650_128.jp2   || Top K similarity values: tensor([[0.7996, 0.7969, 0.7916, 0.7875, 0.7847, 0.7838, 0.7832, 0.7828, 0.7813,\n",
      "         0.7800]], device='mps:0')\n",
      "MATCH: 139 in VL00770_087.jp2   || Top K similarity values: tensor([[0.7549, 0.7547, 0.7545, 0.7448, 0.7442, 0.7437, 0.7437, 0.7428, 0.7422,\n",
      "         0.7410]], device='mps:0')\n",
      "MATCH: 140 in VL04528_076.jp2   || Top K similarity values: tensor([[0.7283, 0.7242, 0.7104, 0.7084, 0.7049, 0.7045, 0.7027, 0.7027, 0.6997,\n",
      "         0.6996]], device='mps:0')\n",
      "MATCH: 141 in VL0H599_166.jp2   || Top K similarity values: tensor([[0.7422, 0.7341, 0.7202, 0.7179, 0.7176, 0.7153, 0.7129, 0.7124, 0.7090,\n",
      "         0.7086]], device='mps:0')\n",
      "######## in VL00022_311.jp2   || Top K similarity values: tensor([[0.7346, 0.7241, 0.7223, 0.7198, 0.7180, 0.7176, 0.7152, 0.7145, 0.7140,\n",
      "         0.7092]], device='mps:0')\n",
      "MATCH: 142 in VL01630_055.jp2   || Top K similarity values: tensor([[0.7638, 0.7624, 0.7567, 0.7518, 0.7501, 0.7472, 0.7468, 0.7462, 0.7441,\n",
      "         0.7394]], device='mps:0')\n",
      "MATCH: 143 in VL0H628_183.jp2   || Top K similarity values: tensor([[0.7961, 0.7884, 0.7779, 0.7335, 0.7061, 0.6991, 0.6987, 0.6926, 0.6920,\n",
      "         0.6916]], device='mps:0')\n",
      "MATCH: 144 in VL0H563_124.jp2   || Top K similarity values: tensor([[0.7399, 0.7327, 0.7302, 0.7251, 0.7219, 0.7201, 0.7195, 0.7193, 0.7178,\n",
      "         0.7174]], device='mps:0')\n",
      "######## in VL03610_059.jp2   || Top K similarity values: tensor([[0.7219, 0.7069, 0.7056, 0.7055, 0.6981, 0.6978, 0.6943, 0.6943, 0.6943,\n",
      "         0.6932]], device='mps:0')\n",
      "MATCH: 145 in VL03967_028.jp2   || Top K similarity values: tensor([[0.7278, 0.7196, 0.7092, 0.7075, 0.7031, 0.7024, 0.6998, 0.6980, 0.6975,\n",
      "         0.6968]], device='mps:0')\n",
      "MATCH: 146 in VL65450_045.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "MATCH: 147 in VL03739_136.jp2   || Top K similarity values: tensor([[0.7767, 0.7681, 0.7662, 0.7555, 0.7538, 0.7496, 0.7413, 0.7412, 0.7410,\n",
      "         0.7381]], device='mps:0')\n",
      "MATCH: 148 in VL0H524_225.jp2   || Top K similarity values: tensor([[0.7287, 0.7168, 0.7163, 0.7135, 0.7134, 0.7107, 0.7074, 0.7055, 0.7054,\n",
      "         0.7018]], device='mps:0')\n",
      "######## in VL0H703_129.jp2   || Top K similarity values: tensor([[0.7760, 0.7744, 0.7705, 0.7599, 0.7535, 0.7535, 0.7473, 0.7448, 0.7422,\n",
      "         0.7420]], device='mps:0')\n",
      "######## in VL03586_091.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL01027_165.jp2   || Top K similarity values: tensor([[0.7530, 0.7461, 0.7388, 0.7380, 0.7365, 0.7364, 0.7316, 0.7315, 0.7308,\n",
      "         0.7306]], device='mps:0')\n",
      "######## in VLA0652_038.jp2   || Top K similarity values: tensor([[0.7715, 0.7709, 0.7630, 0.7558, 0.7516, 0.7428, 0.7420, 0.7367, 0.7363,\n",
      "         0.7356]], device='mps:0')\n",
      "MATCH: 149 in VL00780_066.jp2   || Top K similarity values: tensor([[0.7351, 0.7294, 0.7248, 0.7204, 0.7183, 0.7141, 0.7112, 0.7096, 0.7082,\n",
      "         0.7054]], device='mps:0')\n",
      "MATCH: 150 in VL04339_088.jp2   || Top K similarity values: tensor([[0.7671, 0.7669, 0.7638, 0.7592, 0.7567, 0.7566, 0.7504, 0.7478, 0.7468,\n",
      "         0.7422]], device='mps:0')\n",
      "######## in VL01300_036.jp2   || Top K similarity values: tensor([[0.7704, 0.7680, 0.7621, 0.7616, 0.7605, 0.7581, 0.7577, 0.7536, 0.7494,\n",
      "         0.7438]], device='mps:0')\n",
      "######## in VL0H948_006.jp2   || Top K similarity values: tensor([[0.7764, 0.7731, 0.7686, 0.7685, 0.7669, 0.7642, 0.7592, 0.7564, 0.7550,\n",
      "         0.7547]], device='mps:0')\n",
      "######## in VL46700_042.jp2   || Top K similarity values: tensor([[0.7444, 0.7339, 0.7299, 0.7296, 0.7236, 0.7236, 0.7229, 0.7205, 0.7192,\n",
      "         0.7180]], device='mps:0')\n",
      "MATCH: 151 in VL01092_234.jp2   || Top K similarity values: tensor([[0.7551, 0.7501, 0.7476, 0.7433, 0.7372, 0.7341, 0.7319, 0.7292, 0.7257,\n",
      "         0.7254]], device='mps:0')\n",
      "MATCH: 152 in VL03541_288.jp2   || Top K similarity values: tensor([[0.7966, 0.7712, 0.7696, 0.7694, 0.7690, 0.7681, 0.7653, 0.7648, 0.7612,\n",
      "         0.7578]], device='mps:0')\n",
      "MATCH: 153 in VL65450_046.jp2   || Top K similarity values: tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], device='mps:0')\n",
      "######## in VL03258_006.jp2   || Top K similarity values: tensor([[0.7159, 0.7109, 0.7104, 0.7036, 0.7024, 0.7001, 0.6999, 0.6987, 0.6978,\n",
      "         0.6966]], device='mps:0')\n",
      "MATCH: 154 in VL00001_240.jp2   || Top K similarity values: tensor([[0.7638, 0.7638, 0.7594, 0.7569, 0.7546, 0.7523, 0.7514, 0.7498, 0.7430,\n",
      "         0.7424]], device='mps:0')\n",
      "MATCH: 155 in VL0H599_164.jp2   || Top K similarity values: tensor([[0.7374, 0.7335, 0.7222, 0.7211, 0.7205, 0.7185, 0.7180, 0.7168, 0.7142,\n",
      "         0.7111]], device='mps:0')\n",
      "MATCH: 156 in VL02946_159.jp2   || Top K similarity values: tensor([[0.7681, 0.7543, 0.7526, 0.7523, 0.7502, 0.7371, 0.7361, 0.7359, 0.7357,\n",
      "         0.7352]], device='mps:0')\n",
      "MATCH: 157 in VL00636_122.jp2   || Top K similarity values: tensor([[0.7791, 0.7579, 0.7454, 0.7414, 0.7384, 0.7381, 0.7342, 0.7338, 0.7330,\n",
      "         0.7325]], device='mps:0')\n",
      "######## in VL04787_106.jp2   || Top K similarity values: tensor([[0.7906, 0.7877, 0.7760, 0.7724, 0.7641, 0.7615, 0.7614, 0.7570, 0.7567,\n",
      "         0.7566]], device='mps:0')\n",
      "Match: 157/231\n",
      "Failed files: ['VL0H703_131.jp2', 'VL0H522_093.jp2', 'VL50950_176.jp2', 'VL0H876_078.jp2', 'VL03999_086.jp2', 'VL01681_063.jp2', 'VL04787_082.jp2', 'VL00736_199.jp2', 'VL02316_117.jp2', 'VLH1003_135.jp2', 'VL49600_061.jp2', 'VL0H638_016.jp2', 'VLH1167_104.jp2', 'VL00034_202.jp2', 'VL03713_044.jp2', 'VL00963_150.jp2', 'VL00768_115.jp2', 'VL03482_019.jp2', 'VL00625_272.jp2', 'VL40450_179.jp2', 'VL00024_071.jp2', 'VL02425_126.jp2', 'VL53700_045.jp2', 'VL02863_055.jp2', 'VL49200_135.jp2', 'VL00815_200.jp2', 'VL0H703_140.jp2', 'VL00578_136.jp2', 'VL42250_085.jp2', 'VL02579_151.jp2', 'VLH1214_034.jp2', 'VL03852_041.jp2', 'VL00503_123.jp2', 'VL01624_034.jp2', 'VL04295_099.jp2', 'VL41050_004.jp2', 'VL04490_048.jp2', 'VL00564_453.jp2', 'VL01582_088.jp2', 'VL0H907_210.jp2', 'VL00575_096.jp2', 'VL01290_111.jp2', 'VL0H703_175.jp2', 'VL00562_149.jp2', 'VL00633_403.jp2', 'VL0H703_174.jp2', 'VL0H313_132.jp2', 'VL01474_048.jp2', 'VL00040_167.jp2', 'VL0H905_266.jp2', 'VL08759_073.jp2', 'VL00815_192.jp2', 'VL49250_050.jp2', 'VL02451_151.jp2', 'VL58650_065.jp2', 'VL0H516_162.jp2', 'VL00001_278.jp2', 'VL00959_187.jp2', 'VL00736_230.jp2', 'VL03610_061.jp2', 'VL08676_060.jp2', 'VL69400_145.jp2', 'VL55650_128.jp2', 'VL00022_311.jp2', 'VL03610_059.jp2', 'VL0H703_129.jp2', 'VL03586_091.jp2', 'VL01027_165.jp2', 'VLA0652_038.jp2', 'VL01300_036.jp2', 'VL0H948_006.jp2', 'VL46700_042.jp2', 'VL03258_006.jp2', 'VL04787_106.jp2']\n"
     ]
    }
   ],
   "source": [
    "# source_dir = \"../data/models\"\n",
    "source_dir = \"/Users/ilerisoy/Downloads/Classics/models\"\n",
    "sub_files = os.listdir(source_dir)\n",
    "\n",
    "# Initialize the vars to keep track of stats\n",
    "match = 0\n",
    "ds_strore_count = 0\n",
    "failed_files = []\n",
    "for file in sub_files:\n",
    "    if file == \".DS_Store\":\n",
    "        ds_strore_count += 1\n",
    "        continue\n",
    "    # print(f\"{file}\")\n",
    "\n",
    "    # Get the path to the folder containing the images\n",
    "    image_path = os.path.join(source_dir, file)\n",
    "\n",
    "    # Load the images from the folder\n",
    "    image = open_image(image_path, convert_mode=convert_mode)\n",
    "    \n",
    "    # Get cloth segmentation mask\n",
    "    segmented_image = get_segmentation_mask(image, seg_processor, seg_model)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    segmented_image = segmented_image.cpu().numpy()\n",
    "    segmented_image = np.array(segmented_image, dtype=np.uint8)\n",
    "\n",
    "    # Create a 3-channel mask\n",
    "    segmented_image_3ch = np.stack([segmented_image] * 3, axis=-1)\n",
    "\n",
    "    # Apply the mask to the input image\n",
    "    filtered_image_np = np.where(segmented_image_3ch == 255, np.array(image), 0)\n",
    "\n",
    "    # Convert the filtered image back to PIL format\n",
    "    filtered_image = Image.fromarray(filtered_image_np, mode='RGB')\n",
    "\n",
    "    # # Save the filtered image\n",
    "    # filtered_image.save(f\"../data/filtered_images/{file[:-4]}_filtered.jpg\")\n",
    "\n",
    "    # # Display the filtered image\n",
    "    # Image._show(filtered_image)\n",
    "\n",
    "    # # Display the segmented image\n",
    "    # plt.imshow(segmented_image)\n",
    "\n",
    "    # Embed the image\n",
    "    image_features = image_encoder(filtered_image, CLIP_model, transform=CLIP_transform, save_folder=\"\", filename=file)\n",
    "\n",
    "    # Do cosine similarity with the design embeddings\n",
    "    similarities = [torch.nn.functional.cosine_similarity(image_features, t) for t in design_embeddings]\n",
    "    similarities = torch.stack(similarities)\n",
    "    \n",
    "    # Get the index of the most k similar designs\n",
    "    k = 10\n",
    "    top_k_similarities = similarities.T.topk(k)\n",
    "\n",
    "    # Get the design labels of the top k similar designs\n",
    "    top_k_design_labels = [design_labels[i] for i in top_k_similarities.indices[0]]\n",
    "\n",
    "    temp_match = match\n",
    "    for design_label in top_k_design_labels:\n",
    "        # print(f\"Design label: {design_label[:7]}\")\n",
    "        # print(f\"File: {file[:7]}\")\n",
    "        if design_label[:6] == file[:6]:\n",
    "            match += 1\n",
    "            print(f\"MATCH: {match} in {file}   || Top K similarity values: {top_k_similarities.values}\")\n",
    "            # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "            # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "            break\n",
    "    \n",
    "    if temp_match == match:\n",
    "        print(f\"######## in {file}   || Top K similarity values: {top_k_similarities.values}\")\n",
    "        failed_files.append(file)\n",
    "        # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "\n",
    "\n",
    "print(f\"Match: {match}/{len(sub_files)-ds_strore_count}\")\n",
    "print(f\"Failed files: {failed_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
