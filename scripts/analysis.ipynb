{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import clip\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(source_dir):\n",
    "    \"\"\"\n",
    "    Renames all jpg files in the source directory with their Design Labels.\n",
    "\n",
    "    Parameters:\n",
    "    source_dir: str, the path to the directory containing the jpg files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    \n",
    "    for file in files:\n",
    "\n",
    "        # Check if the file is a jpg\n",
    "        if file.endswith('.jpg'):\n",
    "\n",
    "            # Get the file extension\n",
    "            _, ext = os.path.splitext(file)\n",
    "\n",
    "            # Skip the first VL in the file name\n",
    "            first_vl_index = file.find('VL')\n",
    "\n",
    "            # Find the next VL in the file name\n",
    "            if first_vl_index != -1:\n",
    "                start_index = file.find('VL', first_vl_index + 2)\n",
    "                if start_index != -1:\n",
    "                    end_index = file.find('.', start_index)\n",
    "                    new_name = file[start_index:end_index] if end_index != -1 else file[start_index:]\n",
    "\n",
    "                    # Rename the file\n",
    "                    original_file_path = os.path.join(source_dir, file)\n",
    "                    new_file_path = os.path.join(source_dir, new_name + ext)\n",
    "                    os.rename(original_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(image_path, convert_mode):\n",
    "    \"\"\"\n",
    "    Opens an image from the given path.\n",
    "\n",
    "    Parameters:\n",
    "    image_path: str, the path to the image.\n",
    "    convert_mode: str, the mode to convert the image to. Options are \"RGB\" and \"L\".\n",
    "\n",
    "    Returns:\n",
    "    image: Image, the opened image.\n",
    "    \"\"\"\n",
    "\n",
    "    assert convert_mode in [\"RGB\", \"L\"], \"Invalid convert mode. Options are 'RGB' and 'L'.\"\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to specified mode\n",
    "    image = image.convert(convert_mode)\n",
    "\n",
    "    return image\n",
    "\n",
    "def display_image(image):\n",
    "    \"\"\"\n",
    "    Displays the image.\n",
    "\n",
    "    Parameters:\n",
    "    image: Image, the image to display.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    image.show()\n",
    "\n",
    "def get_palette(num_cls):\n",
    "    \"\"\" Returns the color map for visualizing the segmentation mask.\n",
    "    Args:\n",
    "        num_cls: Number of classes\n",
    "    Returns:\n",
    "        The color map\n",
    "    \"\"\"\n",
    "    n = num_cls\n",
    "    palette = [0] * (n * 3)\n",
    "    for j in range(0, n):\n",
    "        lab = j\n",
    "        palette[j * 3 + 0] = 0\n",
    "        palette[j * 3 + 1] = 0\n",
    "        palette[j * 3 + 2] = 0\n",
    "        i = 0\n",
    "        while lab:\n",
    "            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
    "            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
    "            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
    "            i += 1\n",
    "            lab >>= 3\n",
    "    return palette\n",
    "\n",
    "\n",
    "def resize_image_to_tensor(image):\n",
    "    \"\"\"\n",
    "    Resizes a given RGB PIL image into a 1x3x336x336 tensor.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to resize.\n",
    "\n",
    "    Returns:\n",
    "    - tensor: torch.Tensor, the resized image as a 1x3x336x336 tensor.\n",
    "    \"\"\"\n",
    "    # Define the transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((336, 336)),  # Resize to 336x336\n",
    "        transforms.ToTensor(),          # Convert to tensor and normalize to [0, 1]\n",
    "    ])\n",
    "\n",
    "    # Apply the transformation\n",
    "    tensor = transform(image)\n",
    "\n",
    "    # Add batch dimension\n",
    "    tensor = tensor.unsqueeze(0)  # Shape: 1x3x336x336\n",
    "\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_encoder(image, model, transform, save_folder, filename):\n",
    "    \"\"\"\n",
    "    Use CLIP model to encode the image and save the tranformed version.\n",
    "    First resizes the image to 224x224, then normalizes it, and finally encodes it.\n",
    "    Then, encodes the image into a 512-dimensional feature vector.\n",
    "    \n",
    "    \n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to encode.\n",
    "    - model: CLIP model, the model used for encoding.\n",
    "    - transform: CLIP transform, the transformation to required for CLIP.\n",
    "    - save_folder: str, the folder to save the transformed image.\n",
    "    - filename: str, the name of the file to save the transformed image as.\n",
    "\n",
    "    Returns:\n",
    "    - image_features: torch.Tensor, the encoded image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CLIP model\n",
    "    model = model.eval().to(DEVICE)\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Encode the image\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "\n",
    "    # # Ensure the save folder exists\n",
    "    # if not os.path.exists(save_folder):\n",
    "    #     print(f\"Creating folder {save_folder}...\")\n",
    "    #     os.makedirs(save_folder)\n",
    "    \n",
    "    # # Save the transformed image\n",
    "    # save_path = os.path.join(save_folder, filename)\n",
    "    # transformed_image_pil = transforms.ToPILImage()(image.squeeze(0).cpu())\n",
    "    # transformed_image_pil.save(save_path)\n",
    "\n",
    "    return image_features\n",
    "\n",
    "def create_reference_embeddings(source_dir, CLIP_model, CLIP_transform, convert_mode, save_folder):\n",
    "    \"\"\"\n",
    "    Creates the image embeddings for the images in the source directory and saves together with labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir: str, the path to the directory containing the images.\n",
    "    - CLIP_model: CLIP model, the CLIP model to use for encoding.\n",
    "    - CLIP_transform: CLIP transforms, the CLIP transformation to apply to the images.\n",
    "    - convert_mode: str, the mode to convert the image to. Options are \"RGB\" and \"L\".\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of files in the source directory\n",
    "    sub_files = os.listdir(source_dir)\n",
    "\n",
    "    # Initialize the list of image features and labels\n",
    "    design_features_list = []\n",
    "    design_labels_list = []\n",
    "\n",
    "    for file in sub_files:\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "        print(f\"Processing {file}...\")\n",
    "\n",
    "        # Get the path to the folder containing the images\n",
    "        image_path = os.path.join(source_dir, file)\n",
    "\n",
    "        # Load the images from the folder\n",
    "        image = open_image(image_path, convert_mode)\n",
    "\n",
    "        # Embed the image\n",
    "        image_features = image_encoder(image, CLIP_model, CLIP_transform, save_folder, filename=file)\n",
    "\n",
    "        # Append the image features and labels to the lists\n",
    "        design_features_list.append(image_features)\n",
    "        design_labels_list.append(file)\n",
    "\n",
    "    # Save the image features and labels\n",
    "    with open(f'../data/design_embeddings_{convert_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(design_features_list, f)\n",
    "    with open(f'../data/design_labels_{convert_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(design_labels_list, f)\n",
    "\n",
    "def get_segmentation_mask(image, processor, model):\n",
    "    \"\"\"\n",
    "    Function to segment clothes in an image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to segment.\n",
    "    - processor: SegformerImageProcessor object, the processor used to preprocess the image.\n",
    "    - model: AutoModelForSemanticSegmentation object, the model used to segment the image.\n",
    "\n",
    "    Returns:\n",
    "    - pred_seg: torch.Tensor, the segmented image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "    # Create a mask for the labels 4, 5, 6, and 7\n",
    "    mask = (pred_seg == 4) | (pred_seg == 5) | (pred_seg == 6) | (pred_seg == 7) | (pred_seg == 8) | (pred_seg == 16) | (pred_seg == 17)\n",
    "\n",
    "    # Set all other labels to 0\n",
    "    pred_seg[~mask] = 0\n",
    "\n",
    "    # Set the labels 4, 5, 6, and 7 to 255\n",
    "    pred_seg[mask] = 255\n",
    "\n",
    "    return pred_seg\n",
    "    \n",
    "    # plt.imshow(pred_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    \"\"\"\n",
    "    Converts a PIL image to a tensor.\n",
    "\n",
    "    Parameters:\n",
    "    image: PIL Image, the image to convert.\n",
    "\n",
    "    Returns:\n",
    "    tensor: Tensor, the converted tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts a tensor to a PIL image.\n",
    "\n",
    "    Parameters:\n",
    "    tensor: Tensor, the tensor image to convert.\n",
    "\n",
    "    Returns:\n",
    "    image: PIL Image, the converted image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the tensor to an image\n",
    "    image = transforms.ToPILImage()(tensor)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def apply_random_rotation(image_tensor, degrees=30):\n",
    "    \"\"\"\n",
    "    Applies random rotation to the given tensor image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    degrees: int or tuple, range of degrees to select from.\n",
    "\n",
    "    Returns:\n",
    "    rotated_tensor: Tensor, the image tensor with random rotation applied.\n",
    "    \"\"\"\n",
    "    # Create a RandomRotation transform\n",
    "    random_rotation = transforms.RandomRotation(degrees=degrees)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    rotated_tensor = random_rotation(image_tensor)\n",
    "\n",
    "    return rotated_tensor\n",
    "\n",
    "\n",
    "def downsample_and_upsample(image_tensor, downsample_level=5):\n",
    "    \"\"\"\n",
    "    Downsamples an input tensor to a specified level and then upsamples it to the original size.\n",
    "\n",
    "    A proper range for downsample_level is 5 to 10.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    downsample_level: int, the factor by which to downsample.\n",
    "\n",
    "    Returns:\n",
    "    upsampled_tensor: Tensor, the upsampled image tensor.\n",
    "    \"\"\"\n",
    "    # Get the original size of the image tensor\n",
    "    original_size = image_tensor.shape[-2:]\n",
    "\n",
    "    # Calculate the downsampled size\n",
    "    downsampled_size = (original_size[0] // downsample_level, original_size[1] // downsample_level)\n",
    "\n",
    "    # Downsample the image tensor\n",
    "    downsampled_tensor = F.interpolate(image_tensor.unsqueeze(0), size=downsampled_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    # Upsample the image tensor back to the original size\n",
    "    upsampled_tensor = F.interpolate(downsampled_tensor.unsqueeze(0), size=original_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    return upsampled_tensor\n",
    "\n",
    "\n",
    "# def gaussian_blur(image_tensor, kernel_size=5, sigma=1.0):\n",
    "#     \"\"\"\n",
    "#     Applies a Gaussian blur to a given tensor.\n",
    "\n",
    "#     Parameters:\n",
    "#     image_tensor: Tensor, the input image tensor.\n",
    "#     kernel_size: int, the size of the Gaussian kernel.\n",
    "#     sigma: float, the standard deviation of the Gaussian kernel.\n",
    "\n",
    "#     Returns:\n",
    "#     blurred_tensor: Tensor, the blurred image tensor.\n",
    "#     \"\"\"\n",
    "#     # Define the Gaussian blur transform\n",
    "#     gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
    "\n",
    "#     # Apply the Gaussian blur to the image tensor\n",
    "#     blurred_tensor = gaussian_blur(image_tensor)\n",
    "\n",
    "#     return blurred_tensor\n",
    "\n",
    "\n",
    "def random_jpeg_compression(image_tensor, min_quality=30, max_quality=70):\n",
    "    \"\"\"\n",
    "    Applies random JPEG compression with varying levels of quality to simulate artifacts and lower quality in images.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    min_quality: int, the minimum JPEG quality.\n",
    "    max_quality: int, the maximum JPEG quality.\n",
    "\n",
    "    Returns:\n",
    "    compressed_tensor: Tensor, the compressed image tensor.\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "\n",
    "    # Generate a random quality level between min_quality and max_quality\n",
    "    quality = random.randint(min_quality, max_quality)\n",
    "\n",
    "    # Save the PIL image to a bytes buffer with the generated quality level\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format='JPEG', quality=quality)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Load the image back from the bytes buffer\n",
    "    compressed_image = Image.open(buffer)\n",
    "\n",
    "    # Convert the PIL image back to a tensor\n",
    "    compressed_tensor = transforms.ToTensor()(compressed_image)\n",
    "\n",
    "    return compressed_tensor\n",
    "\n",
    "\n",
    "def random_mask(image_tensor, mask_size=500, area_to_mask=4000000):\n",
    "    \"\"\"\n",
    "    Randomly masks out regions of the image tensor.\n",
    "\n",
    "    Proper range for mask_size is 500 to 1000.\n",
    "    and for num_masks is such that in total 4k pixels are masked.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    mask_size: int, the size of the mask.\n",
    "    num_masks: int, the number of masks to apply.\n",
    "\n",
    "    Returns:\n",
    "    masked_tensor: Tensor, the masked image tensor.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the image tensor\n",
    "    _, height, width = image_tensor.shape\n",
    "\n",
    "    # Create a copy of the image tensor to apply masks\n",
    "    masked_tensor = image_tensor.clone()\n",
    "\n",
    "    # Calculate the area of one mask\n",
    "    mask_area = mask_size * mask_size\n",
    "\n",
    "    # Calculate the number of masks needed defaulting to 4M pixels\n",
    "    num_masks = area_to_mask // mask_area\n",
    "\n",
    "    print(f\"Number of masks: {num_masks}\")\n",
    "\n",
    "    for _ in range(num_masks):\n",
    "        # Randomly select the top-left corner of the mask\n",
    "        top = random.randint(0, height - mask_size)\n",
    "        left = random.randint(0, width - mask_size)\n",
    "\n",
    "        # Apply the mask by setting the selected region to zero\n",
    "        masked_tensor[:, top:top + mask_size, left:left + mask_size] = 0\n",
    "\n",
    "    return masked_tensor\n",
    "\n",
    "\n",
    "def random_color_jitter(image_tensor, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1):\n",
    "    \"\"\"\n",
    "    Applies random color jitter to the given tensor image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    brightness: float or tuple, how much to jitter brightness.\n",
    "    contrast: float or tuple, how much to jitter contrast.\n",
    "    saturation: float or tuple, how much to jitter saturation.\n",
    "    hue: float or tuple, how much to jitter hue.\n",
    "\n",
    "    Returns:\n",
    "    jittered_tensor: Tensor, the image tensor with random color jitter applied.\n",
    "    \"\"\"\n",
    "    # Create a ColorJitter transform\n",
    "    color_jitter = transforms.ColorJitter(brightness=brightness, contrast=contrast, saturation=saturation, hue=hue)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    jittered_tensor = color_jitter(image_tensor)\n",
    "\n",
    "    return jittered_tensor\n",
    "\n",
    "\n",
    "def add_synthetic_shadows(image_tensor, num_shadows=3, shadow_intensity=0.5, shadow_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Adds synthetic shadows to an image tensor to mimic uneven lighting conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_tensor (torch.Tensor): The input image tensor with shape (C, H, W).\n",
    "    - num_shadows (int): Number of shadow shapes to add.\n",
    "    - shadow_intensity (float): The intensity of the shadows (0 = no shadow, 1 = completely black).\n",
    "    - shadow_color (tuple): The color of the shadow in RGB.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The image tensor with synthetic shadows.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, H, W = image_tensor.shape\n",
    "    shadow_image = image_tensor.clone()\n",
    "\n",
    "    for _ in range(num_shadows):\n",
    "        # Randomly generate an ellipse\n",
    "        center_x = np.random.randint(0, W)\n",
    "        center_y = np.random.randint(0, H)\n",
    "        axis_x = np.random.randint(W // 8, W // 2)\n",
    "        axis_y = np.random.randint(H // 8, H // 2)\n",
    "        angle = np.random.uniform(0, 180)\n",
    "        angle = torch.tensor(angle)  # Convert angle to a tensor\n",
    "\n",
    "\n",
    "        # Create a meshgrid for the image\n",
    "        Y, X = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "\n",
    "        # Apply the ellipse equation\n",
    "        ellipse = (((X - center_x) * torch.cos(angle) + (Y - center_y) * torch.sin(angle)) ** 2) / axis_x ** 2 + \\\n",
    "                  (((X - center_x) * torch.sin(angle) - (Y - center_y) * torch.cos(angle)) ** 2) / axis_y ** 2\n",
    "\n",
    "        # Create a mask where the ellipse condition is satisfied\n",
    "        mask = ellipse <= 1\n",
    "\n",
    "        # Apply the shadow by reducing the intensity of the masked region\n",
    "        for i in range(3):  # Assuming image is RGB\n",
    "            shadow_image[i][mask] = (shadow_image[i][mask] * (1 - shadow_intensity) + \n",
    "                                      shadow_color[i] * shadow_intensity)\n",
    "\n",
    "    return shadow_image\n",
    "\n",
    "\n",
    "def apply_random_shearing(image_tensor, shear):\n",
    "    \"\"\"\n",
    "    Applies random shearing effect to the given tensor image.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    shear: float or tuple, range of degrees to select from for shearing.\n",
    "\n",
    "    Returns:\n",
    "    sheared_tensor: Tensor, the image tensor with random shearing applied.\n",
    "    \"\"\"\n",
    "    # Create a RandomAffine transform with shearing\n",
    "    random_shearing = transforms.RandomAffine(degrees=0, shear=shear)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    sheared_tensor = random_shearing(image_tensor)\n",
    "\n",
    "    return sheared_tensor\n",
    "\n",
    "\n",
    "def apply_perspective_transform(image_tensor, distortion_scale=0.5, p=1.0):\n",
    "    \"\"\"\n",
    "    Applies perspective transformations to simulate viewing the image from different angles.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    distortion_scale: float, the degree of distortion (0 to 1).\n",
    "    p: float, probability of applying the transformation.\n",
    "\n",
    "    Returns:\n",
    "    transformed_tensor: Tensor, the image tensor with perspective transformation applied.\n",
    "    \"\"\"\n",
    "    # Create a RandomPerspective transform\n",
    "    perspective_transform = transforms.RandomPerspective(distortion_scale=distortion_scale, p=p)\n",
    "\n",
    "    # Apply the transform to the image tensor\n",
    "    transformed_tensor = perspective_transform(image_tensor)\n",
    "\n",
    "    return transformed_tensor\n",
    "\n",
    "\n",
    "def apply_photographic_transformations(image_tensor, gamma_range=(0.8, 1.2), exposure_range=(0.8, 1.2), lighting_direction_range=(0.8, 1.2)):\n",
    "    \"\"\"\n",
    "    Applies transformations like random changes in gamma, exposure, or lighting direction to simulate different photographic conditions.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    gamma_range: tuple, range of gamma values to select from.\n",
    "    exposure_range: tuple, range of exposure values to select from.\n",
    "    lighting_direction_range: tuple, range of lighting direction values to select from.\n",
    "\n",
    "    Returns:\n",
    "    transformed_tensor: Tensor, the image tensor with photographic transformations applied.\n",
    "    \"\"\"\n",
    "    # Apply random gamma correction\n",
    "    gamma = random.uniform(*gamma_range)\n",
    "    gamma_transform = transforms.functional.adjust_gamma(image_tensor, gamma)\n",
    "    \n",
    "    # Apply random exposure adjustment\n",
    "    exposure = random.uniform(*exposure_range)\n",
    "    exposure_transform = transforms.functional.adjust_brightness(gamma_transform, exposure)\n",
    "    \n",
    "    # Apply random lighting direction adjustment (simulated using brightness and contrast)\n",
    "    lighting_direction = random.uniform(*lighting_direction_range)\n",
    "    lighting_transform = transforms.functional.adjust_contrast(exposure_transform, lighting_direction)\n",
    "    \n",
    "    return lighting_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load an image and convert to tensor\n",
    "# image = Image.open('/Users/ilerisoy/Library/CloudStorage/GoogleDrive-mtilerisoy@gmail.com/My Drive/Vlisco/ML-based-Image-Matching/data/designs/VL00815.jpg')\n",
    "# image_tensor = transforms.ToTensor()(image)\n",
    "\n",
    "# # Apply photographic transformations to the image tensor\n",
    "# transformed_tensor = apply_photographic_transformations(image_tensor, gamma_range=(0.8, 1.2), exposure_range=(0.8, 1.2), lighting_direction_range=(0.8, 1.2))\n",
    "\n",
    "# # Convert back to PIL image to visualize\n",
    "# transformed_image = transforms.ToPILImage()(transformed_tensor)\n",
    "# transformed_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilerisoy/anaconda3/envs/pg/lib/python3.8/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VL0H516.jpg...\n",
      "Processing VL00562.jpg...\n",
      "Processing VL03916.jpg...\n",
      "Processing VL00760.jpg...\n",
      "Processing VL49600.jpg...\n",
      "Processing VL58650.jpg...\n",
      "Processing VL08932.jpg...\n",
      "Processing VL44050.jpg...\n",
      "Processing VL00564.jpg...\n",
      "Processing VL54350.jpg...\n",
      "Processing VL08759.jpg...\n",
      "Processing VL03816.jpg...\n",
      "Processing VL03784.jpg...\n",
      "Processing VL02918.jpg...\n",
      "Processing VL03541.jpg...\n",
      "Processing VL03999.jpg...\n",
      "Processing VL48350.jpg...\n",
      "Processing VL73650.jpg...\n",
      "Processing VL8870.jpg...\n",
      "Processing VL04009.jpg...\n",
      "Processing VL2961R.jpg...\n",
      "Processing VLH1167.jpg...\n",
      "Processing VL01201.jpg...\n",
      "Processing VLA0020.jpg...\n",
      "Processing VLS8589.jpg...\n",
      "Processing VL80021.jpg...\n",
      "Processing VL2961Rotated.jpg...\n",
      "Processing VL04490.jpg...\n",
      "Processing VL00815.jpg...\n",
      "Processing VL00633.jpg...\n",
      "Processing VL65450.jpg...\n",
      "Total number of embeddings: 31\n",
      "Type of design embeddings: <class 'list'>\n",
      "Design Labels: ['VL0H516.jpg', 'VL00562.jpg', 'VL03916.jpg', 'VL00760.jpg', 'VL49600.jpg', 'VL58650.jpg', 'VL08932.jpg', 'VL44050.jpg', 'VL00564.jpg', 'VL54350.jpg', 'VL08759.jpg', 'VL03816.jpg', 'VL03784.jpg', 'VL02918.jpg', 'VL03541.jpg', 'VL03999.jpg', 'VL48350.jpg', 'VL73650.jpg', 'VL8870.jpg', 'VL04009.jpg', 'VL2961R.jpg', 'VLH1167.jpg', 'VL01201.jpg', 'VLA0020.jpg', 'VLS8589.jpg', 'VL80021.jpg', 'VL2961Rotated.jpg', 'VL04490.jpg', 'VL00815.jpg', 'VL00633.jpg', 'VL65450.jpg']\n",
      "Length of Design Labels: 31\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"mps\"\n",
    "\n",
    "# Source directory containing the scraped folders\n",
    "designs_dir = \"../data/designs\"\n",
    "\n",
    "convert_mode = \"RGB\"\n",
    "\n",
    "# Load the CLIP model\n",
    "CLIP_model, CLIP_transform = clip.load(\"ViT-L/14@336px\")\n",
    "\n",
    "# Segmentation model initialization\n",
    "seg_processor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "seg_model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "\n",
    "# Create the reference embeddings\n",
    "create_reference_embeddings(designs_dir, CLIP_model, CLIP_transform, convert_mode=convert_mode, save_folder=\"\")\n",
    "\n",
    "# Load the design database embeddings and labels\n",
    "with open(f'../data/design_embeddings_{convert_mode}.pkl', 'rb') as f:\n",
    "    design_embeddings = pickle.load(f)\n",
    "with open(f'../data/design_labels_{convert_mode}.pkl', 'rb') as f:\n",
    "    design_labels = pickle.load(f)\n",
    "\n",
    "print(f'Total number of embeddings: {len(design_embeddings)}')\n",
    "print(f'Type of design embeddings: {type(design_embeddings)}')\n",
    "print(f'Design Labels: {design_labels}')\n",
    "print(f'Length of Design Labels: {len(design_labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create triplets\n",
    "\n",
    "It will take apprx. 1 hour to transform 250 designs -> 12 sec per design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing VL0H516.jpg...\n",
      "Time taken: 0.3835258483886719\n",
      "Top K similarity values: tensor([0.7664, 0.7593, 0.7535, 0.7511, 0.7481], device='mps:0')\n",
      "Top K design labels: ['VL04490.jpg', 'VL48350.jpg', 'VL00633.jpg', 'VL01201.jpg', 'VL73650.jpg']\n"
     ]
    }
   ],
   "source": [
    "def create_triplets(source_dir):\n",
    "    \n",
    "    # List all files in the source directory\n",
    "    sub_files = os.listdir(source_dir)\n",
    "\n",
    "    for file in sub_files:\n",
    "        if file == \".DS_Store\" or file == \".ipynb_checkpoints\":\n",
    "            continue\n",
    "        print(f\"Processing {file}...\")\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Get the path to the folder containing the images\n",
    "        image_path = os.path.join(source_dir, file)\n",
    "\n",
    "        # Load the images from the folder\n",
    "        image = open_image(image_path, convert_mode=convert_mode)\n",
    "\n",
    "        # Convert image to tensor\n",
    "        image_tensor = image_to_tensor(image)\n",
    "\n",
    "        # # Apply random rotation to the image tensor\n",
    "        # rotated_tensor = apply_random_rotation(image_tensor, degrees=30)\n",
    "\n",
    "        # # Apply downsample and upsample to the image tensor\n",
    "        # downsampled_upsampled_tensor = downsample_and_upsample(image_tensor, downsample_level=10)\n",
    "\n",
    "        # # Apply random JPEG compression to the image tensor\n",
    "        # compressed_tensor = random_jpeg_compression(image_tensor, min_quality=30, max_quality=50)\n",
    "\n",
    "        # # Apply random masks to the image tensor\n",
    "        # masked_tensor = random_mask(image_tensor, mask_size=900)\n",
    "\n",
    "        # # Apply color jitter to the image tensor\n",
    "        # jittered_tensor = random_color_jitter(image_tensor, brightness=0.5, contrast=0.5, saturation=0.5, hue=0.1)\n",
    "\n",
    "        # # Add synthetic shadows to the image tensor\n",
    "        # shadowed_tensor = add_synthetic_shadows(image_tensor)\n",
    "\n",
    "        # # Apply perspective transformation to the image tensor\n",
    "        # transformed_tensor = apply_perspective_transform(image_tensor, distortion_scale=0.5)\n",
    "\n",
    "        # # Apply random shearing to the image tensor\n",
    "        # sheared_tensor = apply_random_shearing(image_tensor, shear=20)\n",
    "\n",
    "        # # Apply photographic transformations to the image tensor\n",
    "        # photographic_tensor = apply_photographic_transformations(image_tensor, gamma_range=(0.8, 1.2), exposure_range=(0.8, 1.2), lighting_direction_range=(0.8, 1.2))\n",
    "\n",
    "        print(f\"Time taken: {time.time() - start}\")\n",
    "\n",
    "        # Embed the image\n",
    "        image_features = image_encoder(image, CLIP_model, CLIP_transform)\n",
    "\n",
    "        # Do cosine similarity with the design embeddings\n",
    "        similarities = [torch.nn.functional.cosine_similarity(image_features, t) for t in design_embeddings]\n",
    "        similarities = torch.stack(similarities)\n",
    "        \n",
    "        # Get the index of the most k similar designs\n",
    "        k = 10\n",
    "        top_k_similarities = similarities.T.topk(k)\n",
    "\n",
    "        # Get the design labels of the top k similar designs\n",
    "        top_k_design_labels = [design_labels[i] for i in top_k_similarities.indices[0]]\n",
    "\n",
    "        print(f\"Top K similarity values: {top_k_similarities.values[0,-5:]}\")\n",
    "        print(f\"Top K design labels: {top_k_design_labels[-5:]}\")\n",
    "        \n",
    "        break\n",
    "    \n",
    "\n",
    "create_triplets(designs_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH: 1 in VL0H516.jpg   || Top K similarity values: tensor([[0.7430, 0.7414, 0.7175, 0.7074, 0.7039]], device='mps:0')\n",
      "VLXXXBeatlesCROPPED.png\n",
      "Top K similarity values: tensor([[0.7188, 0.7029, 0.7028, 0.6957, 0.6882]], device='mps:0')\n",
      "VL00562.jpg\n",
      "Top K similarity values: tensor([[0.7314, 0.7084, 0.7044, 0.7041, 0.7018]], device='mps:0')\n",
      "MATCH: 2 in VL03916.jpg   || Top K similarity values: tensor([[0.7601, 0.7437, 0.7270, 0.7234, 0.7125]], device='mps:0')\n",
      "MATCH: 3 in VL00760.jpg   || Top K similarity values: tensor([[0.7787, 0.7510, 0.7396, 0.7393, 0.7377]], device='mps:0')\n",
      "MATCH: 4 in VL49600.jpg   || Top K similarity values: tensor([[0.7247, 0.7187, 0.7180, 0.7148, 0.7144]], device='mps:0')\n",
      "MATCH: 5 in VL58650.jpg   || Top K similarity values: tensor([[0.7906, 0.7355, 0.7337, 0.7320, 0.7284]], device='mps:0')\n",
      "MATCH: 6 in VL08932.jpg   || Top K similarity values: tensor([[0.7355, 0.7245, 0.7115, 0.7007, 0.7003]], device='mps:0')\n",
      "MATCH: 7 in VL44050.jpg   || Top K similarity values: tensor([[0.7374, 0.7197, 0.7172, 0.7028, 0.7002]], device='mps:0')\n",
      "MATCH: 8 in VL00564.jpg   || Top K similarity values: tensor([[0.7237, 0.7230, 0.7149, 0.7113, 0.7016]], device='mps:0')\n",
      "VL54350.jpg\n",
      "Top K similarity values: tensor([[0.7329, 0.7226, 0.7136, 0.7129, 0.7117]], device='mps:0')\n",
      "VL08759.jpg\n",
      "Top K similarity values: tensor([[0.7456, 0.7283, 0.7235, 0.7182, 0.7127]], device='mps:0')\n",
      "MATCH: 9 in VL03816.jpg   || Top K similarity values: tensor([[0.7183, 0.6683, 0.6606, 0.6488, 0.6487]], device='mps:0')\n",
      "MATCH: 10 in VL03784.jpg   || Top K similarity values: tensor([[0.8032, 0.7822, 0.7700, 0.7632, 0.7627]], device='mps:0')\n",
      "VL8870DidiStone.png\n",
      "Top K similarity values: tensor([[0.7517, 0.7418, 0.7302, 0.7250, 0.7155]], device='mps:0')\n",
      "VLXXXBeatles.png\n",
      "Top K similarity values: tensor([[0.6818, 0.6797, 0.6788, 0.6766, 0.6684]], device='mps:0')\n",
      "MATCH: 11 in VL02918.jpg   || Top K similarity values: tensor([[0.7982, 0.7733, 0.7661, 0.7568, 0.7549]], device='mps:0')\n",
      "MATCH: 12 in VL03541.jpg   || Top K similarity values: tensor([[0.7902, 0.7796, 0.7777, 0.7546, 0.7530]], device='mps:0')\n",
      "MATCH: 13 in VL03999.jpg   || Top K similarity values: tensor([[0.7835, 0.7574, 0.7402, 0.7308, 0.7186]], device='mps:0')\n",
      "VL48350.jpg\n",
      "Top K similarity values: tensor([[0.7099, 0.7015, 0.7009, 0.7002, 0.6975]], device='mps:0')\n",
      "MATCH: 14 in VL2961RBBBB.jpg   || Top K similarity values: tensor([[0.7264, 0.7211, 0.7209, 0.7201, 0.7087]], device='mps:0')\n",
      "MATCH: 15 in VL2961RAAAA.jpg   || Top K similarity values: tensor([[0.7114, 0.7044, 0.6951, 0.6942, 0.6777]], device='mps:0')\n",
      "MATCH: 16 in VL73650.jpg   || Top K similarity values: tensor([[0.7639, 0.7530, 0.7328, 0.7238, 0.7165]], device='mps:0')\n",
      "MATCH: 17 in VL04009.jpg   || Top K similarity values: tensor([[0.7962, 0.7279, 0.7202, 0.7005, 0.6956]], device='mps:0')\n",
      "MATCH: 18 in VL2961R.jpg   || Top K similarity values: tensor([[0.7073, 0.6892, 0.6790, 0.6640, 0.6547]], device='mps:0')\n",
      "VLH1167.jpg\n",
      "Top K similarity values: tensor([[0.7343, 0.7269, 0.7092, 0.7023, 0.7011]], device='mps:0')\n",
      "MATCH: 19 in VL01201.jpg   || Top K similarity values: tensor([[0.7549, 0.7351, 0.7259, 0.7210, 0.7198]], device='mps:0')\n",
      "VLA0020.jpg\n",
      "Top K similarity values: tensor([[0.7713, 0.7476, 0.7400, 0.7385, 0.7373]], device='mps:0')\n",
      "MATCH: 20 in VLS8589.jpg   || Top K similarity values: tensor([[0.7569, 0.7350, 0.7238, 0.7231, 0.7218]], device='mps:0')\n",
      "MATCH: 21 in VL80021.jpg   || Top K similarity values: tensor([[0.7869, 0.7839, 0.7692, 0.7509, 0.7371]], device='mps:0')\n",
      "MATCH: 22 in VL04490.jpg   || Top K similarity values: tensor([[0.6805, 0.6741, 0.6634, 0.6599, 0.6520]], device='mps:0')\n",
      "VL00815.jpg\n",
      "Top K similarity values: tensor([[0.7561, 0.7268, 0.7226, 0.7107, 0.7087]], device='mps:0')\n",
      "MATCH: 23 in VL00633.jpg   || Top K similarity values: tensor([[0.7918, 0.7700, 0.7595, 0.7558, 0.7538]], device='mps:0')\n",
      "MATCH: 24 in VL65450.jpg   || Top K similarity values: tensor([[0.7821, 0.7773, 0.7339, 0.7326, 0.7182]], device='mps:0')\n",
      "Match: 24/34\n",
      "Failed files: ['VLXXXBeatlesCROPPED.png', 'VL00562.jpg', 'VL54350.jpg', 'VL08759.jpg', 'VL8870DidiStone.png', 'VLXXXBeatles.png', 'VL48350.jpg', 'VLH1167.jpg', 'VLA0020.jpg', 'VL00815.jpg']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"../data/models\"\n",
    "sub_files = os.listdir(source_dir)\n",
    "\n",
    "# Initialize the vars to keep track of stats\n",
    "match = 0\n",
    "ds_strore_count = 0\n",
    "failed_files = []\n",
    "for file in sub_files:\n",
    "    if file == \".DS_Store\":\n",
    "        ds_strore_count += 1\n",
    "        continue\n",
    "    # print(f\"{file}\")\n",
    "\n",
    "    # Get the path to the folder containing the images\n",
    "    image_path = os.path.join(source_dir, file)\n",
    "\n",
    "    # Load the images from the folder\n",
    "    image = open_image(image_path, convert_mode=convert_mode)\n",
    "    \n",
    "    # Get cloth segmentation mask\n",
    "    segmented_image = get_segmentation_mask(image, seg_processor, seg_model)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    segmented_image = segmented_image.cpu().numpy()\n",
    "    segmented_image = np.array(segmented_image, dtype=np.uint8)\n",
    "\n",
    "    # Create a 3-channel mask\n",
    "    segmented_image_3ch = np.stack([segmented_image] * 3, axis=-1)\n",
    "\n",
    "    # Apply the mask to the input image\n",
    "    filtered_image_np = np.where(segmented_image_3ch == 255, np.array(image), 0)\n",
    "\n",
    "    # Convert the filtered image back to PIL format\n",
    "    filtered_image = Image.fromarray(filtered_image_np, mode='RGB')\n",
    "\n",
    "    # # Save the filtered image\n",
    "    # filtered_image.save(f\"../data/filtered_images/{file[:-4]}_filtered.jpg\")\n",
    "\n",
    "    # # Display the filtered image\n",
    "    # Image._show(filtered_image)\n",
    "\n",
    "    # # Display the segmented image\n",
    "    # plt.imshow(segmented_image)\n",
    "\n",
    "    # Embed the image\n",
    "    image_features = image_encoder(filtered_image, CLIP_model, transform=CLIP_transform, save_folder=\"\", filename=file)\n",
    "\n",
    "    # Do cosine similarity with the design embeddings\n",
    "    similarities = [torch.nn.functional.cosine_similarity(image_features, t) for t in design_embeddings]\n",
    "    similarities = torch.stack(similarities)\n",
    "    # print(f\"Shape of similarities: {similarities.shape}\")\n",
    "    \n",
    "    # Get the index of the most k similar designs\n",
    "    k = 5\n",
    "    top_k_similarities = similarities.T.topk(k)\n",
    "\n",
    "    # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "\n",
    "    # Get the design labels of the top k similar designs\n",
    "    top_k_design_labels = [design_labels[i] for i in top_k_similarities.indices[0]]\n",
    "\n",
    "    # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "    # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "    # print(\"################################\")\n",
    "\n",
    "    temp_match = match\n",
    "    for design_label in top_k_design_labels:\n",
    "        # print(f\"Design label: {design_label[:7]}\")\n",
    "        # print(f\"File: {file[:7]}\")\n",
    "        if design_label[:6] == file[:6]:\n",
    "            match += 1\n",
    "            print(f\"MATCH: {match} in {file}   || Top K similarity values: {top_k_similarities.values}\")\n",
    "            # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "            # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "            break\n",
    "    \n",
    "    if temp_match == match:\n",
    "        print(f\"{file}\")\n",
    "        print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "        failed_files.append(file)\n",
    "        # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "\n",
    "\n",
    "print(f\"Match: {match}/{len(sub_files)-ds_strore_count}\")\n",
    "print(f\"Failed files: {failed_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
