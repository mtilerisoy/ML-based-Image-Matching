{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilerisoy/anaconda3/envs/pg/lib/python3.8/site-packages/transformers/utils/deprecation.py:165: UserWarning: The following named arguments are not valid for `SegformerImageProcessor.__init__` and were ignored: 'feature_extractor_type'\n",
      "  return func(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import clip\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(source_dir):\n",
    "    \"\"\"\n",
    "    Renames all jpg files in the source directory with their Design Labels.\n",
    "\n",
    "    Parameters:\n",
    "    source_dir: str, the path to the directory containing the jpg files.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # List all files in the source directory\n",
    "    files = os.listdir(source_dir)\n",
    "    \n",
    "    for file in files:\n",
    "\n",
    "        # Check if the file is a jpg\n",
    "        if file.endswith('.jpg'):\n",
    "\n",
    "            # Get the file extension\n",
    "            _, ext = os.path.splitext(file)\n",
    "\n",
    "            # Skip the first VL in the file name\n",
    "            first_vl_index = file.find('VL')\n",
    "\n",
    "            # Find the next VL in the file name\n",
    "            if first_vl_index != -1:\n",
    "                start_index = file.find('VL', first_vl_index + 2)\n",
    "                if start_index != -1:\n",
    "                    end_index = file.find('.', start_index)\n",
    "                    new_name = file[start_index:end_index] if end_index != -1 else file[start_index:]\n",
    "\n",
    "                    # Rename the file\n",
    "                    original_file_path = os.path.join(source_dir, file)\n",
    "                    new_file_path = os.path.join(source_dir, new_name + ext)\n",
    "                    os.rename(original_file_path, new_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(image_path, convert_mode):\n",
    "    \"\"\"\n",
    "    Opens an image from the given path.\n",
    "\n",
    "    Parameters:\n",
    "    image_path: str, the path to the image.\n",
    "    convert_mode: str, the mode to convert the image to. Options are \"RGB\" and \"L\".\n",
    "\n",
    "    Returns:\n",
    "    image: Image, the opened image.\n",
    "    \"\"\"\n",
    "\n",
    "    assert convert_mode in [\"RGB\", \"L\"], \"Invalid convert mode. Options are 'RGB' and 'L'.\"\n",
    "    \n",
    "    # Open the image\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "    # Convert the image to specified mode\n",
    "    image = image.convert(convert_mode)\n",
    "\n",
    "    return image\n",
    "\n",
    "def display_image(image):\n",
    "    \"\"\"\n",
    "    Displays the image.\n",
    "\n",
    "    Parameters:\n",
    "    image: Image, the image to display.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    image.show()\n",
    "\n",
    "def create_reference_embeddings(source_dir, CLIP_model, CLIP_transform, convert_mode):\n",
    "    \"\"\"\n",
    "    Creates the image embeddings for the images in the source directory and saves together with labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - source_dir: str, the path to the directory containing the images.\n",
    "    - CLIP_model: CLIP model, the CLIP model to use for encoding.\n",
    "    - CLIP_transform: CLIP transforms, the CLIP transformation to apply to the images.\n",
    "    - convert_mode: str, the mode to convert the image to. Options are \"RGB\" and \"L\".\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the list of files in the source directory\n",
    "    sub_files = os.listdir(source_dir)\n",
    "\n",
    "    # Initialize the list of image features and labels\n",
    "    design_features_list = []\n",
    "    design_labels_list = []\n",
    "\n",
    "    for file in sub_files:\n",
    "        if file == \".DS_Store\":\n",
    "            continue\n",
    "        print(f\"Processing {file}...\")\n",
    "\n",
    "        # Get the path to the folder containing the images\n",
    "        image_path = os.path.join(source_dir, file)\n",
    "\n",
    "        # Load the images from the folder\n",
    "        image = open_image(image_path, convert_mode)\n",
    "\n",
    "        # Embed the image\n",
    "        image_features = image_encoder(image, CLIP_model, CLIP_transform)\n",
    "\n",
    "        # Append the image features and labels to the lists\n",
    "        design_features_list.append(image_features)\n",
    "        design_labels_list.append(file)\n",
    "\n",
    "    # Save the image features and labels\n",
    "    with open(f'../data/design_embeddings_{convert_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(design_features_list, f)\n",
    "    with open(f'../data/design_labels_{convert_mode}.pkl', 'wb') as f:\n",
    "        pickle.dump(design_labels_list, f)\n",
    "\n",
    "def get_palette(num_cls):\n",
    "    \"\"\" Returns the color map for visualizing the segmentation mask.\n",
    "    Args:\n",
    "        num_cls: Number of classes\n",
    "    Returns:\n",
    "        The color map\n",
    "    \"\"\"\n",
    "    n = num_cls\n",
    "    palette = [0] * (n * 3)\n",
    "    for j in range(0, n):\n",
    "        lab = j\n",
    "        palette[j * 3 + 0] = 0\n",
    "        palette[j * 3 + 1] = 0\n",
    "        palette[j * 3 + 2] = 0\n",
    "        i = 0\n",
    "        while lab:\n",
    "            palette[j * 3 + 0] |= (((lab >> 0) & 1) << (7 - i))\n",
    "            palette[j * 3 + 1] |= (((lab >> 1) & 1) << (7 - i))\n",
    "            palette[j * 3 + 2] |= (((lab >> 2) & 1) << (7 - i))\n",
    "            i += 1\n",
    "            lab >>= 3\n",
    "    return palette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def crop_image(image, model):\n",
    "    \"\"\"\n",
    "    Crops an image by detecting humans using a YOLO model.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to crop.\n",
    "    - model: YOLO object, the YOLO model to use for detection.\n",
    "\n",
    "    Returns:\n",
    "    - cropped_images: list, the cropped images. Each element is a PIL.Image object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure the image is in RGB format and an Image object\n",
    "    assert isinstance(image, Image.Image), \"image must be a PIL.Image object\"\n",
    "    assert image.mode == \"RGB\", \"Image is not in RGB format\"\n",
    "\n",
    "    # # Ensure the image is in RGB format\n",
    "    # image = image.convert(\"RGB\")\n",
    "\n",
    "    # Resize the image to make its dimensions divisible by 32\n",
    "    new_width = (image.width // 32) * 32\n",
    "    new_height = (image.height // 32) * 32\n",
    "    image = image.resize((new_width, new_height))\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    img = torch.from_numpy(np.array(image)).float()\n",
    "    img /= 255.0  # Normalize the image\n",
    "    img = img.permute((2, 0, 1)).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Inference\n",
    "    results = model(img, verbose=False)\n",
    "    boxes = results[0].boxes.xyxy.cpu().tolist()\n",
    "    classes = results[0].boxes.cls.cpu().tolist()\n",
    "\n",
    "    cropped_images = []\n",
    "    \n",
    "    for box, cls in zip(boxes, classes):\n",
    "        # Get the class name from the model class dictionary\n",
    "        class_name = model.names[cls]\n",
    "\n",
    "        if class_name == 'person' or class_name == 'surfboard' or class_name == 'tie' or class_name == 'tennis racket' or class_name == 'sports ball' or class_name == 'frisbee':\n",
    "            # Crop the image\n",
    "            crop_img = image.crop((box[0], box[1], box[2], box[3]))\n",
    "            cropped_images.append(crop_img)\n",
    "        else:\n",
    "            print(f\"Skipping {class_name}\")\n",
    "            continue\n",
    "    return cropped_images\n",
    "\n",
    "def image_encoder(image, model, transform):\n",
    "    \"\"\"\n",
    "    Use CLIP model to encode the image.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to encode.\n",
    "\n",
    "    Returns:\n",
    "    - image_features: torch.Tensor, the encoded image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the CLIP model\n",
    "    model = model.eval().to(DEVICE)\n",
    "\n",
    "    # Preprocess the image\n",
    "    image = transform(image).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Encode the image\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    return image_features\n",
    "\n",
    "def get_segmentation_mask(image, processor, model):\n",
    "    \"\"\"\n",
    "    Function to segment clothes in an image.\n",
    "\n",
    "    Parameters:\n",
    "    - image: PIL.Image object, the image to segment.\n",
    "    - processor: SegformerImageProcessor object, the processor used to preprocess the image.\n",
    "    - model: AutoModelForSemanticSegmentation object, the model used to segment the image.\n",
    "\n",
    "    Returns:\n",
    "    - pred_seg: torch.Tensor, the segmented image.\n",
    "    \"\"\"\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits.cpu()\n",
    "\n",
    "    upsampled_logits = nn.functional.interpolate(\n",
    "        logits,\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "    # Create a mask for the labels 4, 5, 6, and 7\n",
    "    mask = (pred_seg == 4) | (pred_seg == 5) | (pred_seg == 6) | (pred_seg == 7) | (pred_seg == 8) | (pred_seg == 16) | (pred_seg == 17)\n",
    "\n",
    "    # Set all other labels to 0\n",
    "    pred_seg[~mask] = 0\n",
    "\n",
    "    # Set the labels 4, 5, 6, and 7 to 255\n",
    "    pred_seg[mask] = 255\n",
    "\n",
    "    return pred_seg\n",
    "    \n",
    "    # plt.imshow(pred_seg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_tensor(image):\n",
    "    \"\"\"\n",
    "    Converts a PIL image to a tensor.\n",
    "\n",
    "    Parameters:\n",
    "    image: PIL Image, the image to convert.\n",
    "\n",
    "    Returns:\n",
    "    tensor: Tensor, the converted tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the image to a tensor\n",
    "    tensor = transforms.ToTensor()(image)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "def tensor_to_image(tensor):\n",
    "    \"\"\"\n",
    "    Converts a tensor to a PIL image.\n",
    "\n",
    "    Parameters:\n",
    "    tensor: Tensor, the tensor image to convert.\n",
    "\n",
    "    Returns:\n",
    "    image: PIL Image, the converted image.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the tensor to an image\n",
    "    image = transforms.ToPILImage()(tensor)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "def downsample_and_upsample(image_tensor, downsample_level=5):\n",
    "    \"\"\"\n",
    "    Downsamples an input tensor to a specified level and then upsamples it to the original size.\n",
    "\n",
    "    A proper range for downsample_level is 5 to 10.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    downsample_level: int, the factor by which to downsample.\n",
    "\n",
    "    Returns:\n",
    "    upsampled_tensor: Tensor, the upsampled image tensor.\n",
    "    \"\"\"\n",
    "    # Get the original size of the image tensor\n",
    "    original_size = image_tensor.shape[-2:]\n",
    "\n",
    "    # Calculate the downsampled size\n",
    "    downsampled_size = (original_size[0] // downsample_level, original_size[1] // downsample_level)\n",
    "\n",
    "    # Downsample the image tensor\n",
    "    downsampled_tensor = F.interpolate(image_tensor.unsqueeze(0), size=downsampled_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    # Upsample the image tensor back to the original size\n",
    "    upsampled_tensor = F.interpolate(downsampled_tensor.unsqueeze(0), size=original_size, mode='bilinear', align_corners=False).squeeze(0)\n",
    "\n",
    "    return upsampled_tensor\n",
    "\n",
    "\n",
    "def gaussian_blur(image_tensor, kernel_size=5, sigma=1.0):\n",
    "    \"\"\"\n",
    "    Applies a Gaussian blur to a given tensor.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    kernel_size: int, the size of the Gaussian kernel.\n",
    "    sigma: float, the standard deviation of the Gaussian kernel.\n",
    "\n",
    "    Returns:\n",
    "    blurred_tensor: Tensor, the blurred image tensor.\n",
    "    \"\"\"\n",
    "    # Define the Gaussian blur transform\n",
    "    gaussian_blur = transforms.GaussianBlur(kernel_size=kernel_size, sigma=sigma)\n",
    "\n",
    "    # Apply the Gaussian blur to the image tensor\n",
    "    blurred_tensor = gaussian_blur(image_tensor)\n",
    "\n",
    "    return blurred_tensor\n",
    "\n",
    "\n",
    "def random_jpeg_compression(image_tensor, min_quality=30, max_quality=70):\n",
    "    \"\"\"\n",
    "    Applies random JPEG compression with varying levels of quality to simulate artifacts and lower quality in images.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    min_quality: int, the minimum JPEG quality.\n",
    "    max_quality: int, the maximum JPEG quality.\n",
    "\n",
    "    Returns:\n",
    "    compressed_tensor: Tensor, the compressed image tensor.\n",
    "    \"\"\"\n",
    "    # Convert the tensor to a PIL image\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "\n",
    "    # Generate a random quality level between min_quality and max_quality\n",
    "    quality = random.randint(min_quality, max_quality)\n",
    "\n",
    "    # Save the PIL image to a bytes buffer with the generated quality level\n",
    "    buffer = io.BytesIO()\n",
    "    image.save(buffer, format='JPEG', quality=quality)\n",
    "    buffer.seek(0)\n",
    "\n",
    "    # Load the image back from the bytes buffer\n",
    "    compressed_image = Image.open(buffer)\n",
    "\n",
    "    # Convert the PIL image back to a tensor\n",
    "    compressed_tensor = transforms.ToTensor()(compressed_image)\n",
    "\n",
    "    return compressed_tensor\n",
    "\n",
    "\n",
    "def random_mask(image_tensor, mask_size=500, area_to_mask=4000000):\n",
    "    \"\"\"\n",
    "    Randomly masks out regions of the image tensor.\n",
    "\n",
    "    Proper range for mask_size is 500 to 1000.\n",
    "    and for num_masks is such that in total 4k pixels are masked.\n",
    "\n",
    "    Parameters:\n",
    "    image_tensor: Tensor, the input image tensor.\n",
    "    mask_size: int, the size of the mask.\n",
    "    num_masks: int, the number of masks to apply.\n",
    "\n",
    "    Returns:\n",
    "    masked_tensor: Tensor, the masked image tensor.\n",
    "    \"\"\"\n",
    "    # Get the dimensions of the image tensor\n",
    "    _, height, width = image_tensor.shape\n",
    "\n",
    "    # Create a copy of the image tensor to apply masks\n",
    "    masked_tensor = image_tensor.clone()\n",
    "\n",
    "    # Calculate the area of one mask\n",
    "    mask_area = mask_size * mask_size\n",
    "\n",
    "    # Calculate the number of masks needed defaulting to 4M pixels\n",
    "    num_masks = area_to_mask // mask_area\n",
    "\n",
    "    print(f\"Number of masks: {num_masks}\")\n",
    "\n",
    "    for _ in range(num_masks):\n",
    "        # Randomly select the top-left corner of the mask\n",
    "        top = random.randint(0, height - mask_size)\n",
    "        left = random.randint(0, width - mask_size)\n",
    "\n",
    "        # Apply the mask by setting the selected region to zero\n",
    "        masked_tensor[:, top:top + mask_size, left:left + mask_size] = 0\n",
    "\n",
    "    return masked_tensor\n",
    "\n",
    "\n",
    "def add_synthetic_shadows(image_tensor, num_shadows=3, shadow_intensity=0.5, shadow_color=(0, 0, 0)):\n",
    "    \"\"\"\n",
    "    Adds synthetic shadows to an image tensor to mimic uneven lighting conditions.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_tensor (torch.Tensor): The input image tensor with shape (C, H, W).\n",
    "    - num_shadows (int): Number of shadow shapes to add.\n",
    "    - shadow_intensity (float): The intensity of the shadows (0 = no shadow, 1 = completely black).\n",
    "    - shadow_color (tuple): The color of the shadow in RGB.\n",
    "\n",
    "    Returns:\n",
    "    - torch.Tensor: The image tensor with synthetic shadows.\n",
    "    \"\"\"\n",
    "    \n",
    "    _, H, W = image_tensor.shape\n",
    "    shadow_image = image_tensor.clone()\n",
    "\n",
    "    for _ in range(num_shadows):\n",
    "        # Randomly generate an ellipse\n",
    "        center_x = np.random.randint(0, W)\n",
    "        center_y = np.random.randint(0, H)\n",
    "        axis_x = np.random.randint(W // 8, W // 2)\n",
    "        axis_y = np.random.randint(H // 8, H // 2)\n",
    "        angle = np.random.uniform(0, 180)\n",
    "        angle = torch.tensor(angle)  # Convert angle to a tensor\n",
    "\n",
    "\n",
    "        # Create a meshgrid for the image\n",
    "        Y, X = torch.meshgrid(torch.arange(H), torch.arange(W), indexing='ij')\n",
    "\n",
    "        # Apply the ellipse equation\n",
    "        ellipse = (((X - center_x) * torch.cos(angle) + (Y - center_y) * torch.sin(angle)) ** 2) / axis_x ** 2 + \\\n",
    "                  (((X - center_x) * torch.sin(angle) - (Y - center_y) * torch.cos(angle)) ** 2) / axis_y ** 2\n",
    "\n",
    "        # Create a mask where the ellipse condition is satisfied\n",
    "        mask = ellipse <= 1\n",
    "\n",
    "        # Apply the shadow by reducing the intensity of the masked region\n",
    "        for i in range(3):  # Assuming image is RGB\n",
    "            shadow_image[i][mask] = (shadow_image[i][mask] * (1 - shadow_intensity) + \n",
    "                                      shadow_color[i] * shadow_intensity)\n",
    "\n",
    "    return shadow_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image and convert to tensor\n",
    "image = Image.open('/Users/ilerisoy/Library/CloudStorage/GoogleDrive-mtilerisoy@gmail.com/My Drive/Vlisco/ML-based-Image-Matching/data/designs/VL00815.jpg')\n",
    "image_tensor = transforms.ToTensor()(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Downsample and upsample the image tensor\n",
    "downsampled_upsampled_tensor = downsample_and_upsample(image_tensor, downsample_level=10)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Convert back to PIL image to visualize\n",
    "downsampled_upsampled_image = transforms.ToPILImage()(downsampled_upsampled_tensor)\n",
    "downsampled_upsampled_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Apply random JPEG compression to the image tensor\n",
    "compressed_tensor = random_jpeg_compression(image_tensor, min_quality=30, max_quality=50)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Convert back to PIL image to visualize\n",
    "compressed_image = transforms.ToPILImage()(compressed_tensor)\n",
    "compressed_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of masks: 4\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Apply random masks to the image tensor\n",
    "masked_tensor = random_mask(image_tensor, mask_size=900)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Convert back to PIL image to visualize\n",
    "masked_image = transforms.ToPILImage()(masked_tensor)\n",
    "masked_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Add synthetic shadows to the image tensor\n",
    "shadowed_tensor = add_synthetic_shadows(image_tensor, shadow_intensity=0.0005, num_shadows=1)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Convert back to PIL image to visualize\n",
    "shadowed_image = transforms.ToPILImage()(shadowed_tensor)\n",
    "shadowed_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Add synthetic shadows to the image tensor\n",
    "shadowed_tensor = add_synthetic_shadows(image_tensor)\n",
    "print(\"Done!\")\n",
    "\n",
    "# Convert back to PIL image to visualize\n",
    "shadowed_image = transforms.ToPILImage()(shadowed_tensor)\n",
    "shadowed_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"mps\"\n",
    "\n",
    "# Source directory containing the scraped folders\n",
    "source_dir = \"../data/designs\"\n",
    "\n",
    "convert_mode = \"RGB\"\n",
    "\n",
    "# Load the CLIP model\n",
    "CLIP_model, CLIP_transform = clip.load(\"ViT-L/14@336px\")\n",
    "\n",
    "# Segmentation model initialization\n",
    "seg_processor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "seg_model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Create the reference embeddings\n",
    "# create_reference_embeddings(source_dir, CLIP_model, CLIP_transform, convert_mode=convert_mode)\n",
    "\n",
    "# # Load the YOLO model\n",
    "# YOLO_model = YOLO(\"yolov10x.pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of embeddings: 31\n",
      "Type of design embeddings: <class 'list'>\n",
      "Design Labels: ['VL0H516.jpg', 'VL00562.jpg', 'VL03916.jpg', 'VL00760.jpg', 'VL49600.jpg', 'VL58650.jpg', 'VL08932.jpg', 'VL44050.jpg', 'VL00564.jpg', 'VL54350.jpg', 'VL08759.jpg', 'VL03816.jpg', 'VL03784.jpg', 'VL02918.jpg', 'VL03541.jpg', 'VL03999.jpg', 'VL48350.jpg', 'VL73650.jpg', 'VL8870.jpg', 'VL04009.jpg', 'VL2961R.jpg', 'VLH1167.jpg', 'VL01201.jpg', 'VLA0020.jpg', 'VLS8589.jpg', 'VL80021.jpg', 'VL2961Rotated.jpg', 'VL04490.jpg', 'VL00815.jpg', 'VL00633.jpg', 'VL65450.jpg']\n",
      "Length of Design Labels: 31\n"
     ]
    }
   ],
   "source": [
    "# Load the design database embeddings and labels\n",
    "with open(f'../data/design_embeddings_{convert_mode}.pkl', 'rb') as f:\n",
    "    design_embeddings = pickle.load(f)\n",
    "with open(f'../data/design_labels_{convert_mode}.pkl', 'rb') as f:\n",
    "    design_labels = pickle.load(f)\n",
    "\n",
    "print(f'Total number of embeddings: {len(design_embeddings)}')\n",
    "print(f'Type of design embeddings: {type(design_embeddings)}')\n",
    "print(f'Design Labels: {design_labels}')\n",
    "print(f'Length of Design Labels: {len(design_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MATCH: 1 in VL0H516.jpg   || Top K similarity values: tensor([[0.7430, 0.7414, 0.7175, 0.7074, 0.7039, 0.6978, 0.6916, 0.6910, 0.6760, 0.6747, 0.6700, 0.6664, 0.6644, 0.6595, 0.6570]], device='mps:0')\n",
      "VLXXXBeatlesCROPPED.png\n",
      "Top K similarity values: tensor([[0.7188, 0.7029, 0.7028, 0.6957, 0.6882, 0.6856, 0.6838, 0.6761, 0.6751, 0.6749, 0.6631, 0.6555, 0.6530, 0.6517, 0.6457]], device='mps:0')\n",
      "MATCH: 2 in VL00562.jpg   || Top K similarity values: tensor([[0.7314, 0.7084, 0.7044, 0.7041, 0.7018, 0.7013, 0.7007, 0.6967, 0.6908, 0.6893, 0.6864, 0.6807, 0.6787, 0.6734, 0.6636]], device='mps:0')\n",
      "MATCH: 3 in VL03916.jpg   || Top K similarity values: tensor([[0.7601, 0.7437, 0.7270, 0.7234, 0.7125, 0.7038, 0.7004, 0.6951, 0.6946, 0.6915, 0.6892, 0.6889, 0.6884, 0.6861, 0.6843]], device='mps:0')\n",
      "MATCH: 4 in VL00760.jpg   || Top K similarity values: tensor([[0.7787, 0.7510, 0.7396, 0.7393, 0.7377, 0.7325, 0.7315, 0.7202, 0.7202, 0.7195, 0.7174, 0.7058, 0.7040, 0.7003, 0.6964]], device='mps:0')\n",
      "MATCH: 5 in VL49600.jpg   || Top K similarity values: tensor([[0.7247, 0.7187, 0.7180, 0.7148, 0.7144, 0.6838, 0.6817, 0.6816, 0.6807, 0.6784, 0.6712, 0.6707, 0.6684, 0.6651, 0.6647]], device='mps:0')\n",
      "MATCH: 6 in VL58650.jpg   || Top K similarity values: tensor([[0.7906, 0.7355, 0.7337, 0.7320, 0.7284, 0.7191, 0.7130, 0.7039, 0.7028, 0.7003, 0.6999, 0.6992, 0.6858, 0.6856, 0.6829]], device='mps:0')\n",
      "MATCH: 7 in VL08932.jpg   || Top K similarity values: tensor([[0.7355, 0.7245, 0.7115, 0.7007, 0.7003, 0.6964, 0.6922, 0.6819, 0.6815, 0.6794, 0.6784, 0.6742, 0.6721, 0.6712, 0.6711]], device='mps:0')\n",
      "MATCH: 8 in VL44050.jpg   || Top K similarity values: tensor([[0.7374, 0.7197, 0.7172, 0.7028, 0.7002, 0.6975, 0.6968, 0.6957, 0.6910, 0.6894, 0.6852, 0.6811, 0.6674, 0.6638, 0.6620]], device='mps:0')\n",
      "MATCH: 9 in VL00564.jpg   || Top K similarity values: tensor([[0.7237, 0.7230, 0.7149, 0.7113, 0.7016, 0.6971, 0.6937, 0.6900, 0.6896, 0.6858, 0.6857, 0.6852, 0.6791, 0.6748, 0.6722]], device='mps:0')\n",
      "MATCH: 10 in VL54350.jpg   || Top K similarity values: tensor([[0.7329, 0.7226, 0.7136, 0.7129, 0.7117, 0.7109, 0.7052, 0.7040, 0.6991, 0.6983, 0.6937, 0.6862, 0.6809, 0.6758, 0.6750]], device='mps:0')\n",
      "MATCH: 11 in VL08759.jpg   || Top K similarity values: tensor([[0.7456, 0.7283, 0.7235, 0.7182, 0.7127, 0.7003, 0.6955, 0.6880, 0.6871, 0.6869, 0.6841, 0.6781, 0.6678, 0.6664, 0.6650]], device='mps:0')\n",
      "MATCH: 12 in VL03816.jpg   || Top K similarity values: tensor([[0.7183, 0.6683, 0.6606, 0.6488, 0.6487, 0.6460, 0.6387, 0.6361, 0.6342, 0.6295, 0.6255, 0.6187, 0.6150, 0.6141, 0.6130]], device='mps:0')\n",
      "MATCH: 13 in VL03784.jpg   || Top K similarity values: tensor([[0.8032, 0.7822, 0.7700, 0.7632, 0.7627, 0.7616, 0.7591, 0.7479, 0.7438, 0.7413, 0.7396, 0.7380, 0.7328, 0.7311, 0.7289]], device='mps:0')\n",
      "VL8870DidiStone.png\n",
      "Top K similarity values: tensor([[0.7517, 0.7418, 0.7302, 0.7250, 0.7155, 0.7133, 0.7090, 0.7076, 0.7065, 0.7029, 0.6987, 0.6986, 0.6841, 0.6826, 0.6809]], device='mps:0')\n",
      "VLXXXBeatles.png\n",
      "Top K similarity values: tensor([[0.6818, 0.6797, 0.6788, 0.6766, 0.6684, 0.6650, 0.6597, 0.6565, 0.6525, 0.6439, 0.6424, 0.6389, 0.6354, 0.6334, 0.6305]], device='mps:0')\n",
      "MATCH: 14 in VL02918.jpg   || Top K similarity values: tensor([[0.7982, 0.7733, 0.7661, 0.7568, 0.7549, 0.7512, 0.7396, 0.7359, 0.7276, 0.7268, 0.7240, 0.7178, 0.7169, 0.7127, 0.7095]], device='mps:0')\n",
      "MATCH: 15 in VL03541.jpg   || Top K similarity values: tensor([[0.7902, 0.7796, 0.7777, 0.7546, 0.7530, 0.7490, 0.7488, 0.7403, 0.7401, 0.7363, 0.7283, 0.7221, 0.7207, 0.7193, 0.7165]], device='mps:0')\n",
      "MATCH: 16 in VL03999.jpg   || Top K similarity values: tensor([[0.7835, 0.7574, 0.7402, 0.7308, 0.7186, 0.7163, 0.7114, 0.7039, 0.7035, 0.7021, 0.7006, 0.6962, 0.6942, 0.6909, 0.6902]], device='mps:0')\n",
      "MATCH: 17 in VL48350.jpg   || Top K similarity values: tensor([[0.7099, 0.7015, 0.7009, 0.7002, 0.6975, 0.6966, 0.6919, 0.6899, 0.6858, 0.6843, 0.6805, 0.6787, 0.6730, 0.6672, 0.6653]], device='mps:0')\n",
      "MATCH: 18 in VL2961RBBBB.jpg   || Top K similarity values: tensor([[0.7264, 0.7211, 0.7209, 0.7201, 0.7087, 0.7026, 0.6936, 0.6899, 0.6866, 0.6858, 0.6850, 0.6839, 0.6772, 0.6760, 0.6722]], device='mps:0')\n",
      "MATCH: 19 in VL2961RAAAA.jpg   || Top K similarity values: tensor([[0.7114, 0.7044, 0.6951, 0.6942, 0.6777, 0.6627, 0.6585, 0.6489, 0.6486, 0.6485, 0.6467, 0.6379, 0.6372, 0.6328, 0.6322]], device='mps:0')\n",
      "MATCH: 20 in VL73650.jpg   || Top K similarity values: tensor([[0.7639, 0.7530, 0.7328, 0.7238, 0.7165, 0.7157, 0.7048, 0.7042, 0.7001, 0.6926, 0.6842, 0.6837, 0.6812, 0.6779, 0.6778]], device='mps:0')\n",
      "MATCH: 21 in VL04009.jpg   || Top K similarity values: tensor([[0.7962, 0.7279, 0.7202, 0.7005, 0.6956, 0.6924, 0.6879, 0.6859, 0.6839, 0.6817, 0.6779, 0.6722, 0.6714, 0.6700, 0.6565]], device='mps:0')\n",
      "MATCH: 22 in VL2961R.jpg   || Top K similarity values: tensor([[0.7073, 0.6892, 0.6790, 0.6640, 0.6547, 0.6525, 0.6437, 0.6421, 0.6336, 0.6301, 0.6266, 0.6236, 0.6235, 0.6180, 0.6159]], device='mps:0')\n",
      "VLH1167.jpg\n",
      "Top K similarity values: tensor([[0.7343, 0.7269, 0.7092, 0.7023, 0.7011, 0.6905, 0.6891, 0.6885, 0.6866, 0.6841, 0.6806, 0.6799, 0.6785, 0.6742, 0.6719]], device='mps:0')\n",
      "MATCH: 23 in VL01201.jpg   || Top K similarity values: tensor([[0.7549, 0.7351, 0.7259, 0.7210, 0.7198, 0.7155, 0.7142, 0.7113, 0.7096, 0.6950, 0.6928, 0.6926, 0.6902, 0.6823, 0.6817]], device='mps:0')\n",
      "VLA0020.jpg\n",
      "Top K similarity values: tensor([[0.7713, 0.7476, 0.7400, 0.7385, 0.7373, 0.7357, 0.7200, 0.7140, 0.7131, 0.7059, 0.7020, 0.7018, 0.7011, 0.6935, 0.6898]], device='mps:0')\n",
      "MATCH: 24 in VLS8589.jpg   || Top K similarity values: tensor([[0.7569, 0.7350, 0.7238, 0.7231, 0.7218, 0.7213, 0.7205, 0.7201, 0.7145, 0.7104, 0.6979, 0.6964, 0.6888, 0.6847, 0.6781]], device='mps:0')\n",
      "MATCH: 25 in VL80021.jpg   || Top K similarity values: tensor([[0.7869, 0.7839, 0.7692, 0.7509, 0.7371, 0.7364, 0.7359, 0.7240, 0.7210, 0.7183, 0.7148, 0.7083, 0.7046, 0.7014, 0.6934]], device='mps:0')\n",
      "MATCH: 26 in VL04490.jpg   || Top K similarity values: tensor([[0.6805, 0.6741, 0.6634, 0.6599, 0.6520, 0.6509, 0.6474, 0.6419, 0.6370, 0.6348, 0.6322, 0.6286, 0.6248, 0.6216, 0.6207]], device='mps:0')\n",
      "VL00815.jpg\n",
      "Top K similarity values: tensor([[0.7561, 0.7268, 0.7226, 0.7107, 0.7087, 0.7052, 0.6942, 0.6898, 0.6846, 0.6805, 0.6794, 0.6790, 0.6787, 0.6762, 0.6761]], device='mps:0')\n",
      "MATCH: 27 in VL00633.jpg   || Top K similarity values: tensor([[0.7918, 0.7700, 0.7595, 0.7558, 0.7538, 0.7523, 0.7452, 0.7410, 0.7284, 0.7259, 0.7231, 0.7145, 0.7142, 0.7126, 0.7113]], device='mps:0')\n",
      "MATCH: 28 in VL65450.jpg   || Top K similarity values: tensor([[0.7821, 0.7773, 0.7339, 0.7326, 0.7182, 0.7176, 0.7112, 0.7105, 0.7083, 0.7023, 0.7003, 0.6975, 0.6919, 0.6891, 0.6886]], device='mps:0')\n",
      "Match: 28/34\n",
      "Failed files: ['VLXXXBeatlesCROPPED.png', 'VL8870DidiStone.png', 'VLXXXBeatles.png', 'VLH1167.jpg', 'VLA0020.jpg', 'VL00815.jpg']\n"
     ]
    }
   ],
   "source": [
    "source_dir = \"../data/models\"\n",
    "sub_files = os.listdir(source_dir)\n",
    "\n",
    "# Initialize the vars to keep track of stats\n",
    "match = 0\n",
    "ds_strore_count = 0\n",
    "failed_files = []\n",
    "for file in sub_files:\n",
    "    if file == \".DS_Store\":\n",
    "        ds_strore_count += 1\n",
    "        continue\n",
    "    # print(f\"{file}\")\n",
    "\n",
    "    # Get the path to the folder containing the images\n",
    "    image_path = os.path.join(source_dir, file)\n",
    "\n",
    "    # Load the images from the folder\n",
    "    image = open_image(image_path, convert_mode=convert_mode)\n",
    "    \n",
    "    # Get cloth segmentation mask\n",
    "    segmented_image = get_segmentation_mask(image, seg_processor, seg_model)\n",
    "\n",
    "    # Convert the tensor to a numpy array\n",
    "    segmented_image = segmented_image.cpu().numpy()\n",
    "    segmented_image = np.array(segmented_image, dtype=np.uint8)\n",
    "\n",
    "    # Create a 3-channel mask\n",
    "    segmented_image_3ch = np.stack([segmented_image] * 3, axis=-1)\n",
    "\n",
    "    # Apply the mask to the input image\n",
    "    filtered_image_np = np.where(segmented_image_3ch == 255, np.array(image), 0)\n",
    "\n",
    "    # Convert the filtered image back to PIL format\n",
    "    filtered_image = Image.fromarray(filtered_image_np, mode='RGB')\n",
    "\n",
    "    # Save the filtered image\n",
    "    filtered_image.save(f\"../data/filtered_images/{file[:-4]}_filtered.jpg\")\n",
    "\n",
    "    # # Display the filtered image\n",
    "    # Image._show(filtered_image)\n",
    "\n",
    "    # # Display the segmented image\n",
    "    # plt.imshow(segmented_image)\n",
    "\n",
    "    # Embed the image\n",
    "    image_features = image_encoder(filtered_image, CLIP_model, CLIP_transform)\n",
    "\n",
    "    # Do cosine similarity with the design embeddings\n",
    "    similarities = [torch.nn.functional.cosine_similarity(image_features, t) for t in design_embeddings]\n",
    "    similarities = torch.stack(similarities)\n",
    "    # print(f\"Shape of similarities: {similarities.shape}\")\n",
    "    \n",
    "    # Get the index of the most k similar designs\n",
    "    k = 15\n",
    "    top_k_similarities = similarities.T.topk(k)\n",
    "\n",
    "    # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "\n",
    "    # Get the design labels of the top k similar designs\n",
    "    top_k_design_labels = [design_labels[i] for i in top_k_similarities.indices[0]]\n",
    "\n",
    "    # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "    # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "    # print(\"################################\")\n",
    "\n",
    "    temp_match = match\n",
    "    for design_label in top_k_design_labels:\n",
    "        # print(f\"Design label: {design_label[:7]}\")\n",
    "        # print(f\"File: {file[:7]}\")\n",
    "        if design_label[:6] == file[:6]:\n",
    "            match += 1\n",
    "            print(f\"MATCH: {match} in {file}   || Top K similarity values: {top_k_similarities.values}\")\n",
    "            # print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "            # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "            break\n",
    "    \n",
    "    if temp_match == match:\n",
    "        print(f\"{file}\")\n",
    "        print(f\"Top K similarity values: {top_k_similarities.values}\")\n",
    "        failed_files.append(file)\n",
    "        # print(f\"Top {k} similar designs for image {file}: {top_k_design_labels}\")\n",
    "\n",
    "\n",
    "print(f\"Match: {match}/{len(sub_files)-ds_strore_count}\")\n",
    "print(f\"Failed files: {failed_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
